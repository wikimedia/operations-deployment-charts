docker:
  pull_policy: IfNotPresent
  registry: docker-registry.discovery.wmnet

net_istio:
  controller:
    version: 1.7.0-2
    replicaCount: 2
    resources:
      requests:
        cpu: 200m
        memory: 40Mi
      limits:
        cpu: 1000m
        memory: 400Mi
  webhook:
    version: 1.7.0-2
    replicaCount: 2
core:
  activator:
    version: 1.7.2-7
    replicaCount: 2
    resources:
      requests:
        cpu: 1000m
        memory: 800Mi
      limits:
        cpu: 3000m
        memory: 800Mi
  autoscaler:
    version: 1.7.2-7
    replicaCount: 2
    resources:
      requests:
        cpu: 1000m
        memory: 100Mi
      limits:
        cpu: 3000m
        memory: 600Mi
  controller:
    version: 1.7.2-7
    replicaCount: 2
    resources:
      requests:
        cpu: 1000m
        memory: 100Mi
      limits:
        cpu: 3000m
        memory: 1000Mi
  webhook:
    # We raise the max number of pods in production since sometimes,
    # when a new knative-serving chart version is rolled out, there is the chance
    # that the webhook pods are not avaiable to answer validation requests
    # from the Kube API (that in turn is contacted by Helm).
    # Having 4 pods helped during the last deployment issue.
    replicaCount: 4
    version: 1.7.2-7
    resources:
      requests:
        cpu: 500m
        memory: 100Mi
      limits:
        cpu: 1000m
        memory: 500Mi
  domain_mapping:
    version: 1.7.2-7
  domain_mapping_webhook:
    version: 1.7.2-7


  config_autoscaler:
    scale-down-delay: "15m"
    # The Activator pods can be placed between Ingress and Target pods (like
    # Kserve ones) to buffer traffic in case a surge of requests happens.
    # It turns out that figuring out the exact sweet spot is not easy, plus
    # it adds more complexity to the request path.
    # Setting this value to zero forces the Activator pods to be placed in
    # the request path only for scale-to-zero environments (where revisions
    # can have zero serving pods, waiting for requests to happen).
    # More info https://knative.dev/docs/serving/load-balancing/target-burst-capacity/
    target-burst-capacity: "0"
    # Percentage of avg concurrent (if configured as metric, otherwise rps etc..)
    # requests to initiate a scale up action. Basically knative tries to scale
    # up earlier to smooth the transition time to more capacity (to avoid overloading
    # the other instances). Default is 70.
    # More info: https://knative.dev/docs/serving/autoscaling/concurrency/#target-utilization
    container-concurrency-target-percentage: "85"

  config_gc:
    # Immediately clean up old Knative revisions not active anymore.
    # More info: https://knative.dev/docs/serving/revisions/revision-admin-config-options/#garbage-collection
    max-non-active-revisions: "0"
    min-non-active-revisions: "0"
    retain-since-create-time: "disabled"
    retain-since-last-active-time: "disabled"
