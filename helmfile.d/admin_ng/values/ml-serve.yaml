# cluster_group is used to identify a group of similar clusters (like for one in eqiad and one in codfw)
# that share some config (values) in "admin_ng/values/<cluster_group>.yaml"
cluster_group: ml-serve

# Defaults applied to all ML clusters
GlobalNetworkPolicies:
  # Allow icmp for all pods and all directions. Useful in debugging
  allow-all-icmp:
    namespaceSelector: 'has(projectcalico.org/name) && projectcalico.org/name != "kube-system"'
    types:
      - Ingress
      - Egress
    ingress:
      - action: Allow
        protocol: ICMP
      - action: Allow
        protocol: ICMPv6
    egress:
      - action: Allow
        protocol: ICMP
      - action: Allow
        protocol: ICMPv6
  default-deny:
    namespaceSelector: 'has(projectcalico.org/name) && projectcalico.org/name != "kube-system"'
    types:
      - Ingress
      - Egress
    egress:
      # Allow all namespaces to communicate to DNS pods
      - action: Allow
        protocol: UDP
        destination:
          services:
            name: "kube-dns"
            namespace: "kube-system"
  # This allows egress from all pods to all pods. Ingress still needs to be allowed by the destination, though.
  allow-pod-to-pod:
    namespaceSelector: 'has(projectcalico.org/name) && projectcalico.org/name != "kube-system"'
    types:
      - Egress
    egress:
      - action: Allow
        destination:
          nets:
            # eqiad
            - "10.67.16.0/21"
            # codfw
            - "10.194.16.0/21"
      - action: Allow
        destination:
          nets:
            # eqiad
            - "2620:0:861:300::/64"
            # codfw
            - "2620:0:860:300::/64"

# Context: https://knative.dev/docs/serving/tag-resolution/
docker:
  registry_cidrs:
      - '10.2.2.44/32'
      - '10.2.1.44/32'

deployExtraClusterRoles:
  - "kserve"
  - "liftwing-debugging"
  - "amdgpu-node-labeller"

# List all namespaces that should be created in every ML Serve cluster.
# For info about what overrides are available, please check ./common.yaml.
namespaces:
  kube-system:
    systemNamespace: true
    allowCriticalPods: true
    pspClusterRole: allow-privileged-psp
    labels:
      istio-injection: disabled
      webhooks.knative.dev/exclude: "true"
  istio-system:
    systemNamespace: true
    allowCriticalPods: true
    labels:
      istio-injection: disabled
  knative-serving:
    systemNamespace: true
    allowCriticalPods: true
    deployTLSCertificate: true
    # See helmfile_namespace_certs.yaml for more info
    # The helmfile config deploys, by default, a TLS cert
    # with hostname == namespace for every non-system namespace.
    # In our case, we have only one istio TLS config that is deployed
    # via knative-serving.
    # Note: We rely on ChangeProp to call LiftWing upon certain
    # Kafka events are emitted. Due to how nodejs works
    # (see https://github.com/nodejs/node/issues/37104) it is
    # not possible to call our inference endpoints with
    # custom HTTP Host headers, unless there are some special
    # SANs in the TLS certificate to support them.
    tlsHostnames:
      - inference
    tlsExtraSANs:
      - '*.revscoring-editquality-goodfaith.wikimedia.org'
      - '*.revscoring-editquality-damaging.wikimedia.org'
      - '*.revscoring-editquality-reverted.wikimedia.org'
      - '*.revscoring-draftquality.wikimedia.org'
      - '*.revscoring-drafttopic.wikimedia.org'
      - '*.revscoring-articlequality.wikimedia.org'
      - '*.revscoring-articletopic.wikimedia.org'
      - '*.experimental.wikimedia.org'
      - '*.articletopic-outlink.wikimedia.org'
      - '*.article-descriptions.wikimedia.org'
      - '*.revertrisk.wikimedia.org'
      - '*.llm.wikimedia.org'
      - '*.logo-detection.wikimedia.org'
      - '*.article-models.wikimedia.org'
      - '*.revision-models.wikimedia.org'
      - '*.edit-check.wikimedia.org'
      - '*.revise-tone-task-generator.wikimedia.org'
    labels:
      istio-injection: disabled
  kserve:
    systemNamespace: true
    allowCriticalPods: true
    labels:
      control-plane: kserve-controller-manager
      controller-tools.k8s.io: "1.0"
      istio-injection: disabled
      webhooks.knative.dev/exclude: "true"
  cert-manager:
    systemNamespace: true
    allowCriticalPods: true
    labels:
      istio-injection: disabled
      webhooks.knative.dev/exclude: "true"
  external-services:
    systemNamespace: true
    deployTLSCertificate: false
  revscoring-editquality-goodfaith:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
    resourcequota:
      pods: {}
      compute:
        requests:
          cpu: "300"
          memory: "240Gi"
        limits:
          cpu: "300"
          memory: "240Gi"
  revscoring-editquality-damaging:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
    resourcequota:
      pods: {}
      compute:
        requests:
          cpu: "300"
          memory: "240Gi"
        limits:
          cpu: "300"
          memory: "240Gi"
  revscoring-editquality-reverted:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
  revscoring-draftquality:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
  revscoring-articlequality:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
    resourcequota:
      pods: {}
      compute:
        requests:
          cpu: "140"
          memory: "100Gi"
        limits:
          cpu: "140"
          memory: "100Gi"
  revscoring-articletopic:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
  revscoring-drafttopic:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
  articletopic-outlink:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
  revise-tone-task-generator:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
  revertrisk:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
    resourcequota:
      pods: {}
      compute:
        requests:
          cpu: "140"
          memory: "200Gi"
        limits:
          cpu: "140"
          memory: "200Gi"
    limitranges:
      container:
        min:
          memory: "100Mi"
          cpu: "100m"
        max:
          memory: "20Gi"
          cpu: "12"
        defaultRequest:
          memory: "100Mi"
          cpu: "100m"
        default:
          memory: "100Mi"
          cpu: "100m"
      pod:
        min:
          memory: "100Mi"
          cpu: "100m"
        max:
          memory: "20Gi"
          cpu: "12"
  readability:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
  article-descriptions:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
    limitranges:
      container:
        min:
          memory: "100Mi"
          cpu: "100m"
        max:
          memory: "40Gi"
          cpu: "16"
        defaultRequest:
          memory: "100Mi"
          cpu: "100m"
        default:
          memory: "100Mi"
          cpu: "100m"
      pod:
        min:
          memory: "100Mi"
          cpu: "100m"
        max:
          memory: "44Gi"
          cpu: "24"
  article-models:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
  revision-models:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
    limitranges:
      container:
        min:
          memory: "100Mi"
          cpu: "100m"
        max:
          memory: "8Gi"
          cpu: "30"
        defaultRequest:
          memory: "100Mi"
          cpu: "100m"
        default:
          memory: "100Mi"
          cpu: "100m"
      pod:
        min:
          memory: "100Mi"
          cpu: "100m"
        max:
          memory: "10Gi"
          cpu: "34"
    resourcequota:
      pods: { }
      compute:
        requests:
          cpu: "200"
          memory: "150Gi"
        limits:
          cpu: "200"
          memory: "150Gi"
  logo-detection:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
  ores-legacy:
    deployClusterRole: deploy
    deployTLSCertificate: true
    # NOTE: istio-injection is disabled here because we want to keep comparibility
    # with servivesd deployed to wikikube (that don't use istio-proxy but tls-proxy).
    labels:
      istio-injection: disabled
      webhooks.knative.dev/exclude: "true"
    tlsExtraSANs:
      - 'ores.wikimedia.org'
      - 'ores-legacy.wikimedia.org'
  recommendation-api-ng:
    deployClusterRole: deploy
    deployTLSCertificate: true
    # NOTE: istio-injection is disabled here because we want to keep comparibility
    # with servivesd deployed to wikikube (that don't use istio-proxy but tls-proxy).
    labels:
      istio-injection: disabled
      webhooks.knative.dev/exclude: "true"
  llm:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
  edit-check:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
    limitranges:
      container:
        min:
          memory: "50Mi"
          cpu: "100m"
        max:
          memory: "8Gi"
          cpu: "8"
        defaultRequest:
          memory: "100Mi"
          cpu: "100m"
        default:
          memory: "100Mi"
          cpu: "100m"
      pod:
        min:
          memory: "50Mi"
          cpu: "100m"
        max:
          memory: "15Gi"
          cpu: "10"
  # The experimental namespace allows the deployment of model servers
  # that are not ready for production yet. After an incubation period,
  # a model server may be promoted to production in its own dedicated
  # k8s namespace.
  experimental:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
    limitranges:
      container:
        min:
          memory: "100Mi"
          cpu: "100m"
        max:
          memory: "40Gi"
          cpu: "16"
        defaultRequest:
          memory: "100Mi"
          cpu: "100m"
        default:
          memory: "100Mi"
          cpu: "100m"
      pod:
        min:
          memory: "100Mi"
          cpu: "100m"
        max:
          memory: "44Gi"
          cpu: "24"
    resourcequota:
      pods: {}
      compute:
        requests:
          cpu: "90"
          memory: "150Gi"
        limits:
          cpu: "90"
          memory: "150Gi"

net_istio:
  ingressgateway:
    servers:
    # The 'hosts' field correspond to the list of backend routes that
    # Knative/Istio will allow/configure on the Gateway. For example,
    # if the FQDN for a Kfserving backend is something-test.example.com,
    # we can allow it in multiple ways:
    # 1) More specific: 'something-test.example.com'
    # 2) Less specific: '*.example.com'
    # 3) All traffic allowed: '*'
    # For the moment option 3) is fine, but we'll need to review the choice.
    - hosts:
        - '*'
      port:
        name: https
        number: 443
        protocol: HTTPS
      tls:
        mode: SIMPLE
        minProtocolVersion: 'TLSV1_2'
        # Please note:
        # This corresponds to the name of a TLS Secret deployed
        # to the istio-system namespace. We deploy it via
        # helmfile_namespace_certs.yaml.
        credentialName: knative-serving-tls-certificate
  mesh:
    service_entries:
      mediawiki-api-proxied-domains:
        spec:
        # This list is needed to instruct the Istio sidecar that
        # HTTP connections related to these domains on port X shouldn't
        # be forwarded to their real upstreams, but handled via
        # the virtual service config.
        # As of April 2024 we connect from our model servers to
        # the MediaWiki API in the following way:
        # 1) We create a TCP connection to the discovery endpoint.
        # 2) We set the right HTTP Host header
        # Due to how Istio HTTP routing works, all the domains listed in
        # the Host headers need to be allowed by ServiceEntry settings.
        # In case we'll use the Istio dns-proxy, we'll not need to
        # resolve and DNS query about these, hence the NONE setting below.
        # More info: https://istio.io/latest/docs/ops/configuration/traffic-management/traffic-routing/
          hosts: &mediawiki_api_domains
          - '*.wikipedia.org'
          - 'wikipedia.org'
          - '*.wiktionary.org'
          - 'wiktionary.org'
          - '*.wikidata.org'
          - 'wikidata.org'
          - '*.wikiquote.org'
          - 'wikiquote.org'
          - '*.wikibooks.org'
          - 'wikibooks.org'
          - '*.wikisource.org'
          - 'wikisource.org'
          - 'commons.wikimedia.org'
          - 'upload.wikimedia.org'
          ports:
          # We need to keep two ports since we support services explicitly
          # setting a proxy (like a discovery endpoint and port) or simply
          # the upstream domain directly (like en.wikipedia.org)
          - name: http-port-k8s
            number: 4680
            protocol: HTTP
          - name: http-port
            number: 80
            protocol: HTTP
          resolution: NONE
      mw-api-int-ro:
        spec:
          hosts:
          - mw-api-int-ro.discovery.wmnet
          ports:
          - name: http-port-k8s
            # This port is used by the Destination Rule for TLS egress origination
            number: 4680
            protocol: HTTP
            targetPort: 4446
          - name: http-port
            # This port is used by the Destination Rule for TLS egress origination
            number: 80
            protocol: HTTP
            targetPort: 4446
          - name: https-port
            number: 4446
            protocol: HTTPS
          resolution: DNS
      eventgate-main-se:
        spec:
          hosts:
          - eventgate-main.discovery.wmnet
          ports:
          - name: http-port
            # This port is used by the Destination Rule for TLS egress origination
            number: 4480
            protocol: HTTP
            targetPort: 4492
          - name: https-port
            number: 4492
            protocol: HTTPS
          resolution: DNS
      rest-gateway:
        spec:
          hosts:
          - rest-gateway.discovery.wmnet
          ports:
          - name: http-port
            # This port is used by the Destination Rule for TLS egress origination
            number: 4111
            protocol: HTTP
            targetPort: 4113
          - name: https-port
            number: 4113
            protocol: HTTPS
          resolution: DNS
      ml-cache-cassandra:
        spec:
          # NOTE: For cassandra, the service IP (as ooposed to the base host
          # IP) is relevant here, since Cassandra instances only listen on
          # those, so we use those specific hostnames; i.e. ml-cache1001-a
          # instead of ml-cache1001
          hosts: &ml_cache_cassandra_hosts
          - "ml-cache1001-a.eqiad.wmnet"
          - "ml-cache1002-a.eqiad.wmnet"
          - "ml-cache1003-a.eqiad.wmnet"
          - "ml-cache2001-a.codfw.wmnet"
          - "ml-cache2002-a.codfw.wmnet"
          - "ml-cache2003-a.codfw.wmnet"
          ports:
          - name: cassandra-client-port
            number: 9042
            protocol: TLS
          resolution: DNS
    destination_rules:
      https-mw-api-int-ro:
        spec:
          host: mw-api-int-ro.discovery.wmnet
          trafficPolicy:
            # The configuration is related to TLS egress origination.
            # See https://istio.io/v1.9/docs/tasks/traffic-management/egress/egress-tls-origination/#tls-origination-for-egress-traffic
            # The TLS settings will force istio-proxy to pick up the HTTPS port
            # set in the related ServiceEntry.
            portLevelSettings:
            # We need to keep two ports since we support services explicitly
            # setting a proxy (like a discovery endpoint and port) or simply
            # the upstream domain directly (like en.wikipedia.org)
            - port:
                number: 4680
              tls:
                mode: SIMPLE
            - port:
                number: 80
              tls:
                mode: SIMPLE
            connectionPool:
              tcp:
                maxConnections: 100
                connectTimeout: 30s
              http:
                # Rationale (may need to be revisited after upgrading Istio):
                # https://phabricator.wikimedia.org/T320374#8338627
                idleTimeout: 5s
                maxRequestsPerConnection: 1000
                # See retries implemented in the Virtual Service settings
                maxRetries: 0
      https-eventgate-dr:
        spec:
          host: eventgate-main.discovery.wmnet
          trafficPolicy:
            # The configuration is related to TLS egress origination.
            # See https://istio.io/v1.9/docs/tasks/traffic-management/egress/egress-tls-origination/#tls-origination-for-egress-traffic
            # The TLS settings will force istio-proxy to pick up the HTTPS port
            # set in the related ServiceEntry.
            portLevelSettings:
            - port:
                number: 4480
              tls:
                mode: SIMPLE
            connectionPool:
              tcp:
                maxConnections: 50
                connectTimeout: 30s
              http:
                # Rationale (may need to be revisited after upgrading Istio):
                # https://phabricator.wikimedia.org/T320374#8338627
                idleTimeout: 5s
                maxRequestsPerConnection: 1000
                # See retries implemented in the Virtual Service settings
                maxRetries: 0
      https-rest-gateway:
        spec:
          host: rest-gateway.discovery.wmnet
          trafficPolicy:
            # The configuration is related to TLS egress origination.
            # See https://istio.io/v1.9/docs/tasks/traffic-management/egress/egress-tls-origination/#tls-origination-for-egress-traffic
            # The TLS settings will force istio-proxy to pick up the HTTPS port
            # set in the related ServiceEntry.
            portLevelSettings:
            - port:
                number: 4111
              tls:
                mode: SIMPLE
            connectionPool:
              tcp:
                maxConnections: 100
                connectTimeout: 30s
              http:
                # Rationale (may need to be revisited after upgrading Istio):
                # https://phabricator.wikimedia.org/T320374#8338627
                idleTimeout: 5s
                maxRequestsPerConnection: 1000
                # See retries implemented in the Virtual Service settings
                maxRetries: 0
    virtual_services:
      mediawiki-api-vs:
        # NOTE: Please keep mediawiki-api-vs and mediawiki-api-vs-transparent-proxy
        # in sync.
        spec:
          gateways:
          - mesh
          hosts: *mediawiki_api_domains
          http:
          - match:
            - port: 4680
            route:
            - destination:
                host: mw-api-int-ro.discovery.wmnet
            retries:
              # Rationale (may need to be revisited after upgrading Istio):
              # https://phabricator.wikimedia.org/T320374#8338627
              attempts: 3
              retryOn: connect-failure,refused-stream,reset,503
            headers:
              request:
                set:
                  # The TLS egress origination settings (see the related destination
                  # rule's comment) have the side effect of presenting
                  # a 'x-forwarded-proto: http' header to the target API, that in turn
                  # may return a warning in every response. To circumvent this problem,
                  # let's explicitly set the header.
                  x-forwarded-proto: https
      mediawiki-api-vs-transparent-proxy:
        # NOTE: Please keep mediawiki-api-vs and mediawiki-api-vs-transparent-proxy
        # in sync.
        # This VS is aimed to support "transparent" proxy use cases for
        # calls to the MediaWiki API (for example, http://en.wikipedia.org)
        # We cannot use mediawiki-api-vs since the HTTP Route spec allow only
        # one port, so for the time being we need to duplicate it.
        spec:
          gateways:
          - mesh
          hosts: *mediawiki_api_domains
          http:
          - match:
            - port: 80
            route:
            - destination:
                host: mw-api-int-ro.discovery.wmnet
            retries:
              # Rationale (may need to be revisited after upgrading Istio):
              # https://phabricator.wikimedia.org/T320374#8338627
              attempts: 3
              retryOn: connect-failure,refused-stream,reset,503
            headers:
              request:
                set:
                  # The TLS egress origination settings (see the related destination
                  # rule's comment) have the side effect of presenting
                  # a 'x-forwarded-proto: http' header to the target API, that in turn
                  # may return a warning in every response. To circumvent this problem,
                  # let's explicitly set the header.
                  x-forwarded-proto: https
      eventgate-main-vs:
        spec:
          gateways:
          - mesh
          hosts:
          - 'eventgate-main.discovery.wmnet'
          http:
          - match:
            - port: 4480
            route:
            - destination:
                host: eventgate-main.discovery.wmnet
            retries:
              # Rationale (may need to be revisited after upgrading Istio):
              # https://phabricator.wikimedia.org/T320374#8338627
              attempts: 3
              retryOn: connect-failure,refused-stream,reset,503
            headers:
              request:
                set:
                  # The TLS egress origination settings (see the related destination
                  # rule's comment) have the side effect of presenting
                  # a 'x-forwarded-proto: http' header to the target API, that in turn
                  # may return a warning in every response. To circumvent this problem,
                  # let's explicitly set the header.
                  x-forwarded-proto: https
      restgw-vs:
        spec:
          gateways:
          - mesh
          hosts:
          - '*.wikipedia.org'
          - 'wikipedia.org'
          http:
          - match:
            - port: 4111
            route:
            - destination:
                host: rest-gateway.discovery.wmnet
            retries:
              # Rationale (may need to be revisited after upgrading Istio):
              # https://phabricator.wikimedia.org/T320374#8338627
              attempts: 3
              retryOn: connect-failure,refused-stream,reset,503
            headers:
              request:
                set:
                  # The TLS egress origination settings (see the related destination
                  # rule's comment) have the side effect of presenting
                  # a 'x-forwarded-proto: http' header to the target API, that in turn
                  # may return a warning in every response. To circumvent this problem,
                  # let's explicitly set the header.
                  x-forwarded-proto: https
      ml-cache-vs:
        spec:
          hosts: *ml_cache_cassandra_hosts
          gateways:
          - mesh
          tcp:
          - match:
            - port: 9042
            route:
            - destination:
                host: ml-cache-cassandra # This is the ServiceEntry defined above

core:
  config_features:
    # This is needed to be able to configure the Pod's dnsConfig
    # from Kserve's InferenceService resources.
    kubernetes.podspec-dnsconfig: "enabled"

helmVersion: "helm3"

istio:
  gateways:
    ingressgateway:
      ports:
        - 8443
        # knative-local-gateway (not exposed via NodePort),
        # only for internal traffic.
        - 8081

# Measure to deal with https://phabricator.wikimedia.org/T318814
# By default Kubernetes DNS records have 5s TTL, and in a Service Mesh
# like Istio this means causing a lot of DNS queries from Envoy to refresh
# every TTL seconds all the Clusters with STRICT_DNS settings.
coredns:
  rewrite_actions:
    continue:
    - 'ttl exact knative-local-gateway.istio-system.svc.cluster.local. 30'
    - 'ttl regex (.*).discovery.wmnet 30'
    - 'ttl regex (.*)-(predictor|transformer)-default-(.*) 30'
    # Avoid IPv6 lookups
    # Due to an Istio bug, every connection made to an IPv6 address
    # on the current version (1.15.7, April 2024) will end up in a TCP SYN
    # handing until timeout.
    # More info: T353622#9415171
    stop:
    - 'type AAAA A'

kserve:
  ingress:
    local_gateway_service: knative-local-gateway.istio-system.svc.cluster.local
    domain: 'wikimedia.org'

limitranges:
  container:
    max:
      memory: "8Gi"
  pod:
    max:
      memory: "10Gi"

typha:
  resources:
    requests:
      cpu: 500m
      memory: 200Mi
    limits:
      cpu: ~
      memory: 200Mi

kubeControllers:
  resources:
    requests:
      cpu: 500m
      memory: 200Mi
    limits:
      cpu: ~
      memory: 200Mi

PodSecurityStandard:
  disablePSPMutations: true  # Disable PSP mutation, allow all seccomp profiles
  enforce: true              # Enforce the PodSecurityStandard profile "restricted"
  disableRestrictedPSP: true # Disable PSP binding for the restricted PSP
