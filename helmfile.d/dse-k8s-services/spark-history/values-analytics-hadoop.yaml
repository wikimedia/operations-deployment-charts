app:
  requests:
    cpu: 4000m
    memory: 6Gi
  limits:
    cpu: 16000m
    memory: 12Gi

config:
  hdfs:
    dfs.nameservices: analytics-hadoop
    dfs.internal.nameservices: analytics-hadoop
    dfs.client.failover.proxy.provider.analytics-hadoop: org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider
    dfs.ha.namenodes.analytics-hadoop: an-master1003-eqiad-wmnet,an-master1004-eqiad-wmnet
    dfs.namenode.servicerpc-address.analytics-hadoop.an-master1003-eqiad-wmnet: an-master1003.eqiad.wmnet:8040
    dfs.namenode.servicerpc-address.analytics-hadoop.an-master1004-eqiad-wmnet: an-master1004.eqiad.wmnet:8040
    dfs.namenode.rpc-address.analytics-hadoop.an-master1003-eqiad-wmnet: an-master1003.eqiad.wmnet:8020
    dfs.namenode.rpc-address.analytics-hadoop.an-master1004-eqiad-wmnet: an-master1004.eqiad.wmnet:8020
    dfs.namenode.http-address.analytics-hadoop.an-master1003-eqiad-wmnet: an-master1003.eqiad.wmnet:50070
    dfs.namenode.http-address.analytics-hadoop.an-master1004-eqiad-wmnet: an-master1004.eqiad.wmnet:50070

  spark:
    spark.history.kerberos.principal: spark/spark-history.svc.eqiad.wmnet
    spark.ui.proxyRedirectUri: https://yarn.wikimedia.org/
    spark.ui.proxyBase: /spark-history
    spark.history.fs.cleaner.maxAge: 60d

  hadoop:
    fs.defaultFS: hdfs://analytics-hadoop/

ingress:
  gatewayHosts:
    default: "spark-history"

external_services:
  hadoop-master:
  - analytics
  hadoop-worker:
  - analytics
