docker:
  registry: docker-registry.discovery.wmnet

app:
  version: &base-image-version 2025-06-11-123150-aa24e2974c855c9c59c33dcd92b3310330b46272@sha256:9a517c4999dd242a2e1da30943c9b2749140ea01d909abcfa2a737f1b9ecfd2b
  executor_pod_image_version: *base-image-version

service:
  deployment: production

config:
  airflow:
    config:
      datahub:
        cluster: prod
        conn_id: datahub_gms
        enabled: true
      logging:
        remote_log_conn_id: s3_dpe
      smtp:
        smtp_host: mx-out1001.wikimedia.org
    local_settings:
      xcom_sidecar:
        tag: '20250601' # This is a bookworm image build date
  connections:
    s3_dpe:
      extra:
        endpoint_url: https://rgw.eqiad.dpe.anycast.wmnet
        region_name: dpe
    kafka_test_eqiad:
      conn_type: kafka
      host: kafka-test-eqiad.external-services.svc.cluster.local
      port: 9093
      extra:
        group.id: airflow-{{ $.Values.config.airflow.instance_name | replace "_" "-" }}
        ssl.ca.location: /etc/ssl/certs/ca-certificates.crt
        bootstrap.servers: kafka-test-eqiad.external-services.svc.cluster.local:9093
        security.protocol: SSL
    kafka_jumbo_eqiad:
      conn_type: kafka
      host: kafka-jumbo-eqiad.external-services.svc.cluster.local
      port: 9093
      extra:
        group.id: airflow-{{ $.Values.config.airflow.instance_name | replace "_" "-" }}
        ssl.ca.location: /etc/ssl/certs/ca-certificates.crt
        bootstrap.servers: kafka-jumbo-eqiad.external-services.svc.cluster.local:9093
        security.protocol: SSL
    kafka_jumbo_eqiad_external:
      conn_type: kafka
      host: kafka-jumbo1010.eqiad.wmnet
      port: 9093
      extra:
        group.id: airflow-{{ $.Values.config.airflow.instance_name | replace "_" "-" }}
        ssl.ca.location: /etc/ssl/certs/ca-certificates.crt
        bootstrap.servers: kafka-jumbo1010.eqiad.wmnet:9093,kafka-jumbo1011.eqiad.wmnet:9093,kafka-jumbo1012.eqiad.wmnet:9093,kafka-jumbo1013.eqiad.wmnet:9093,kafka-jumbo1014.eqiad.wmnet:9093,kafka-jumbo1015.eqiad.wmnet:9093
        security.protocol: SSL
  oidc:
    idp_server: idp.wikimedia.org

external_services:
  gitsync:
    # git-sync pulls code from our gitlab instance
    gitlab: [wikimedia]
  scheduler:
    # the scheduler needs to be able to send alert emails
    wikimail: [mx]
    # it also needs to be able to move task logs to s3 once they complete
    s3: [eqiad-dpe]
  kerberos:
    # the airflow kerberos compomnent needs to be able to talk to our KDC
    # to get an initial TGT, as well as renew it
    kerberos: [kdc]
  webserver:
    # The webserver needs to talk to CAS to perform the OAuth dance, to log
    # users in
    cas: [idp]
    # The webserver needs to talk to S3 to fetch task logs for completed tasks
    s3: [eqiad-dpe]
  task-pod:
    # Task pods need access to kerberos to get a Client-to-Service ticket from
    # their TGT, by talking to the TGS
    kerberos: [kdc]
    # Some tasks connect to S3 directly
    s3: [eqiad-dpe]
    # Task pods needs to be able to send alert emails
    wikimail: [mx]
    # Task pods have access to both kafka-{test,jumbo}-eqiad by default to interact with datahub
    # and lineage (whether Airflow DAGs or Hive tables)
    kafka: [test-eqiad, jumbo-eqiad]
  hadoop-shell:
    # The hadoop shell needs to authenticate access to hadoop services against Kerberos
    kerberos: [kdc]


gitsync:
  image_tag: 2024-08-22-120818-fbafbcdb385bf1008ba0ac8ee350e9fe411a057d@sha256:3e01121704b405a08649012571aba0ce6834ab3aa3428df0b02a476b7ba4c3f5
  volume:
    storage_class: ceph-cephfs-ssd

kerberos:
  volume:
    storage_class: ceph-cephfs-ssd

worker:
  config:
    hadoop:
      hdfs:
        dfs.cluster.administrators: hdfs analytics-admins,ops
        dfs.hosts.exclude: /etc/hadoop/conf.analytics-hadoop/hosts.exclude
      core:
        ha.zookeeper.quorum: an-conf1004.eqiad.wmnet,an-conf1005.eqiad.wmnet,an-conf1006.eqiad.wmnet
      yarn:
        yarn.resourcemanager.keytab: /etc/security/keytabs/hadoop/yarn.keytab
        yarn.resourcemanager.zk-address: an-conf1004.eqiad.wmnet,an-conf1005.eqiad.wmnet,an-conf1006.eqiad.wmnet
      hive:
        hive.metastore.kerberos.keytab.file: /etc/security/keytabs/hive/hive.keytab
        hive.server2.authentication.kerberos.keytab: /etc/security/keytabs/hive/hive.keytab
    spark:
      spark:
        spark.executorEnv.REQUESTS_CA_BUNDLE: /etc/ssl/certs/ca-certificates.crt
        spark.yarn.appMasterEnv.REQUESTS_CA_BUNDLE: /etc/ssl/certs/ca-certificates.crt
        spark.yarn.historyServer.address: yarn.wikimedia.org

discovery:
  listeners:
  - mw-api-int     # to be able to talk to https://meta.wikimedia.org/w/api.php?action=streamsconfig
  - noc            # to be able to talk to https://noc.wikimedia.org/conf/dblists/xxx
  - schema         # to be able to talk to https://schema.discovery.wmnet
  - analytics-web  # to be able to talk to https://analytics.wikimedia.org
