config:
  airflow:
    dags_folder: analytics_test
    instance_name: analytics-test
    auth:
      role_mappings:
        airflow-analytics-ops: [Op]
    config:
      core:
        executor: KubernetesExecutor
        max_active_tasks_per_dag: 64
        parallelism: 128
      kerberos:
        principal: analytics/airflow-analytics-test.discovery.wmnet
    extra_rbac:
    # This will be used to read the mediawiki dumps job and create a pod from its spec
    - apiGroups: ["batch"]
      resources: ["jobs"]
      verbs: ["get"]
  oidc:
    client_id: airflow_analytics_test

ingress:
  gatewayHosts:
    default: "airflow-analytics-test"
    extraFQDNs:
    - airflow-analytics-test.wikimedia.org

gitsync:
  # Some DAGS in analytics_test import functions from the main and analytics folder
  extra_dags_folders: [main, analytics]

discovery:
  listeners:
  - mw-api-int  # to be able to talk to https://meta.wikimedia.org/w/api.php?action=streamsconfig
  - noc         # to be able to talk to https://noc.wikimedia.org/conf/dblists/xxx
