inference:
  predictor:
    config:
      # These settings are applied at the pod level,
      # that includes all containers not explicitly defined
      # in the isvc (like the istio-{validation,proxy}).
      # The knative-serving ones seem to need
      # an extra level of settings in the knative's control-plane
      # due to how the various pods/revisions are handled
      # (dynamically etc..).
      # More info https://phabricator.wikimedia.org/T369493
      securityContext:
        seccompProfile:
          type: RuntimeDefault

inference_services:
  langid:
    predictor:
      image: "machinelearning-liftwing-inference-services-langid"
      image_version: "2025-05-23-112000-publish"
      config:
        minReplicas: 1
        maxReplicas: 1
      custom_env:
        - name: MODEL_NAME
          value: "langid"
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/llm/langid/20231011160342/"
      container:
        resources:
          limits:
            memory: 2Gi
          requests:
            memory: 2Gi
