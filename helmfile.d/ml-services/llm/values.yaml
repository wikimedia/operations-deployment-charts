docker:
  registry: docker-registry.discovery.wmnet/wikimedia
  imagePullPolicy: IfNotPresent

networkpolicy:
  egress:
    enabled: true
    # These endpoints should be reachable by Istio proxy sidecars.
    dst_nets:
      - cidr: 10.2.1.54/32 # thanos-swift.svc.codfw.wmnet
        ports:
        - port: 443
          protocol: tcp
      - cidr: 10.2.2.54/32 # thanos-swift.svc.eqiad.wmnet
        ports:
        - port: 443
          protocol: tcp

monitoring:
  enabled: true

inference:
  annotations:
    sidecar.istio.io/inject: "true"
    autoscaling.knative.dev/metric: "rps"
    prometheus.kserve.io/scrape: "true"
    prometheus.kserve.io/port: "8080"
    prometheus.kserve.io/path: "/metrics"

inference_services:
  langid:
    annotations:
      autoscaling.knative.dev/target: "50"
    predictor:
      image: "machinelearning-liftwing-inference-services-langid"
      image_version: "2023-10-16-162753-publish"
      config:
        minReplicas: 1
        maxReplicas: 3
      custom_env:
        - name: MODEL_NAME
          value: "langid"
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/llm/langid/20231011160342/"
      container:
        resources:
          limits:
            memory: 2Gi
          requests:
            memory: 2Gi
  nllb-200:
    predictor:
      image: "machinelearning-liftwing-inference-services-llm"
      image_version: "2023-10-19-095650-publish"
      custom_env:
        - name: MODEL_NAME
          value: "nllb-200-distilled-600M"
        - name: LLM_CLASS
          value: "nllb.NLLB"
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/llm/nllb-200-distilled-600M/"
      container:
        resources:
          limits:
            cpu: "4"
            memory: 6Gi
          requests:
            cpu: "4"
            memory: 6Gi
  nllb-200-gpu:
    predictor:
      image: "machinelearning-liftwing-inference-services-llm"
      image_version: "2023-10-19-095650-publish"
      custom_env:
        - name: MODEL_NAME
          value: "nllb-200-distilled-600M"
        - name: LLM_CLASS
          value: "nllb.NLLB"
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/llm/nllb-200-distilled-600M/"
      container:
        resources:
          limits:
            cpu: "4"
            memory: 6Gi
            amd.com/gpu: 1
          requests:
            cpu: "4"
            memory: 6Gi
            amd.com/gpu: 1
