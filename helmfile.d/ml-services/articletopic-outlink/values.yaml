docker:
  registry: docker-registry.discovery.wmnet/wikimedia
  imagePullPolicy: IfNotPresent

networkpolicy:
  egress:
    enabled: true
    # These endpoints should be reachable by Istio proxy sidecars.
    dst_nets:
      - cidr: 10.2.1.54/32 # thanos-swift.svc.codfw.wmnet
        ports:
        - port: 443
          protocol: tcp
      - cidr: 10.2.2.54/32 # thanos-swift.svc.eqiad.wmnet
        ports:
        - port: 443
          protocol: tcp
      - cidr: 10.2.1.22/32 # api-ro.svc.codfw.wmnet
        ports:
        - port: 443
          protocol: tcp
      - cidr: 10.2.2.22/32 # api-ro.svc.eqiad.wmnet
        ports:
        - port: 443
          protocol: tcp
      - cidr: 10.2.1.81/32 # mw-api-int-ro.svc.codfw.wmnet
        ports:
        - port: 4446
          protocol: tcp
      - cidr: 10.2.2.81/32 # mw-api-int-ro.svc.eqiad.wmnet
        ports:
        - port: 4446
          protocol: tcp
      - cidr: 10.2.1.45/32 # eventgate-main.svc.codfw.wmnet
        ports:
        - port: 4492
          protocol: tcp
      - cidr: 10.2.2.45/32 # eventgate-main.svc.eqiad.wmnet
        ports:
        - port: 4492
          protocol: tcp

monitoring:
  enabled: true

inference:
  annotations:
    sidecar.istio.io/inject: "true"
    autoscaling.knative.dev/metric: "rps"
    autoscaling.knative.dev/target: "5"
    prometheus.kserve.io/scrape: "true"
    prometheus.kserve.io/port: "8080"
    prometheus.kserve.io/path: "/metrics"
  predictor:
    image: "machinelearning-liftwing-inference-services-outlink"
    version: "2026-01-26-104505-publish"
    base_env:
      - name: STORAGE_URI
        value: "s3://wmf-ml-models/articletopic/outlink/20221111111111/"
      - name: MODEL_VERSION
        value: "alloutlinks_202209"
      - name: EVENTGATE_URL
        value: "http://eventgate-main.discovery.wmnet:4480/v1/events"
      - name: EVENTGATE_STREAM
        value: "mediawiki.page_outlink_topic_prediction_change.v1"
      - name: WIKI_URL
        value: "http://mw-api-int-ro.discovery.wmnet:4680"
      - name: FORCE_HTTP
        value: "true"

inference_services:
  outlink-topic-model:
    predictor:
      config:
        minReplicas: 2
        maxReplicas: 5
