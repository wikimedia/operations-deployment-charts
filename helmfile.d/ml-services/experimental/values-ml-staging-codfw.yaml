inference_services:
  revertrisk-wikidata:
    predictor:
      image: "machinelearning-liftwing-inference-services-revertrisk-wikidata"
      image_version: "2025-05-23-111959-publish"
      custom_env:
        - name: MODEL_NAME
          value: "revertrisk-wikidata"
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/experimental/revertrisk-wikidata/20230512162400/"
        - name: WIKI_URL
          value: "http://mw-api-int-ro.discovery.wmnet:4680"
      container:
        resources:
          limits:
            cpu: "2"
            memory: 4Gi
          requests:
            cpu: "2"
            memory: 4Gi

  articlequality:
    predictor:
      image: "machinelearning-liftwing-inference-services-articlequality"
      image_version: "2025-10-16-103324-publish"
      custom_env:
        - name: MODEL_NAME
          value: "articlequality"
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/articlequality/language-agnostic/20250425125943/"
        - name: FORCE_HTTP
          value: "True"
      container:
        resources:
          limits:
            memory: 1Gi
          requests:
            memory: 1Gi

  reference-need:
    annotations:
      autoscaling.knative.dev/target: "3"
    predictor:
      config:
        minReplicas: 1
        maxReplicas: 1
      image: "machinelearning-liftwing-inference-services-reference-quality"
      image_version: "2025-03-19-145037-publish"
      custom_env:
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/reference-quality/20250127142109/"
        - name: FORCE_HTTP
          value: "True"
        - name: MODEL_TO_DEPLOY
          value: "reference-need"
        - name: BATCH_SIZE
          value: "16"
          # We set number of threads for each worker model to (num_cpus/2)-1 to leave CPU capacity to the main event loop
        - name: NUM_THREADS
          value: "7"
        - name: NUM_OF_WORKERS
          value: "2"
      container:
        resources:
          limits:
            cpu: "16"
            memory: 4Gi
          requests:
            cpu: "16"
            memory: 4Gi

  reference-risk:
    annotations:
      autoscaling.knative.dev/target: "3"
    predictor:
      config:
        minReplicas: 1
        maxReplicas: 8
      image: "machinelearning-liftwing-inference-services-reference-quality"
      image_version: "2025-05-23-112449-publish"
      custom_env:
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/reference-quality/20250127142109/"
        - name: FORCE_HTTP
          value: "True"
        - name: MODEL_TO_DEPLOY
          value: "reference-risk"
      container:
        resources:
          limits:
            cpu: "4"
            memory: 3Gi
          requests:
            cpu: "4"
            memory: 3Gi


  article-country:
    predictor:
      image: "machinelearning-liftwing-inference-services-article-country"
      image_version: "2025-05-23-112448-publish"
      custom_env:
        - name: MODEL_NAME
          value: "article-country"
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/article-country/20240901015102/"
        - name: FORCE_HTTP
          value: "True"
      container:
        resources:
          limits:
            cpu: "2"
            memory: 2Gi
          requests:
            cpu: "2"
            memory: 2Gi

  edit-check:
    predictor:
      config:
        maxReplicas: 1
        batcher:
          maxBatchSize: 32
          maxLatency: 100
      image: "machinelearning-liftwing-inference-services-edit-check"
      image_version: "2025-06-25-142517-publish"
      custom_env:
        - name: MODEL_NAME
          value: "edit-check-staging"
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/edit-check/peacock/"
        - name: MODEL_VERSION
          value: "v1"
        - name: USE_METADATA
          value: "true"
      container:
        resources:
          limits:
            cpu: "4"
            memory: 8Gi
          requests:
            cpu: "4"
            memory: 8Gi

external_services_app_label_selector: app-wmf
external_services:
  cassandra:
  - ml-cache-a-eqiad
  - ml-cache-a-codfw
