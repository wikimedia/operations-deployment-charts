inference_services:

  revertrisk-wikidata:
    predictor:
      image: "machinelearning-liftwing-inference-services-revertrisk-wikidata"
      image_version: "2025-11-17-105041-publish"
      custom_env:
        - name: MODEL_NAME
          value: "revertrisk-wikidata"
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/revertrisk/wikidata/20251104121312/"
        - name: FORCE_HTTP
          value: "True"
      container:
        resources:
          limits:
            cpu: "2"
            memory: 4Gi
          requests:
            cpu: "2"
            memory: 4Gi

  articlequality:
    predictor:
      image: "machinelearning-liftwing-inference-services-articlequality"
      image_version: "2025-10-16-103324-publish"
      custom_env:
        - name: MODEL_NAME
          value: "articlequality"
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/articlequality/language-agnostic/20250425125943/"
        - name: FORCE_HTTP
          value: "True"
      container:
        resources:
          limits:
            memory: 1Gi
          requests:
            memory: 1Gi

  reference-need:
    annotations:
      autoscaling.knative.dev/target: "3"
    predictor:
      config:
        minReplicas: 1
        maxReplicas: 1
      image: "machinelearning-liftwing-inference-services-reference-quality"
      image_version: "2025-03-19-145037-publish"
      custom_env:
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/reference-quality/20250127142109/"
        - name: FORCE_HTTP
          value: "True"
        - name: MODEL_TO_DEPLOY
          value: "reference-need"
        - name: BATCH_SIZE
          value: "16"
          # We set number of threads for each worker model to (num_cpus/2)-1 to leave CPU capacity to the main event loop
        - name: NUM_THREADS
          value: "7"
        - name: NUM_OF_WORKERS
          value: "2"
      container:
        resources:
          limits:
            cpu: "16"
            memory: 4Gi
          requests:
            cpu: "16"
            memory: 4Gi

  reference-risk:
    annotations:
      autoscaling.knative.dev/target: "3"
    predictor:
      config:
        minReplicas: 1
        maxReplicas: 8
      image: "machinelearning-liftwing-inference-services-reference-quality"
      image_version: "2025-05-23-112449-publish"
      custom_env:
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/reference-quality/20250127142109/"
        - name: FORCE_HTTP
          value: "True"
        - name: MODEL_TO_DEPLOY
          value: "reference-risk"
      container:
        resources:
          limits:
            cpu: "4"
            memory: 3Gi
          requests:
            cpu: "4"
            memory: 3Gi


  article-country:
    predictor:
      image: "machinelearning-liftwing-inference-services-article-country"
      image_version: "2025-05-23-112448-publish"
      custom_env:
        - name: MODEL_NAME
          value: "article-country"
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/article-country/20240901015102/"
        - name: FORCE_HTTP
          value: "True"
      container:
        resources:
          limits:
            cpu: "2"
            memory: 2Gi
          requests:
            cpu: "2"
            memory: 2Gi

  edit-check:
    predictor:
      config:
        maxReplicas: 1
        batcher:
          maxBatchSize: 32
          maxLatency: 100
      image: "machinelearning-liftwing-inference-services-edit-check"
      image_version: "2025-06-25-142517-publish"
      custom_env:
        - name: MODEL_NAME
          value: "edit-check-staging"
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/edit-check/peacock/"
        - name: MODEL_VERSION
          value: "v1"
        - name: USE_METADATA
          value: "true"
      container:
        resources:
          limits:
            cpu: "4"
            memory: 8Gi
          requests:
            cpu: "4"
            memory: 8Gi

  revise-tone-task-generator:
    predictor:
      image: "machinelearning-liftwing-inference-services-revise-tone-task-generator"
      image_version: "2025-12-02-094146-publish"
      custom_env:
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/edit-check/peacock/"
        - name: WIKI_URL
          value: "http://mw-api-int-ro.discovery.wmnet:4680"
        - name: OUTLINK_TOPIC_MODEL_URL
          value: "http://outlink-topic-model.articletopic-outlink/v1/models/outlink-topic-model:predict"
        - name: OUTLINK_TOPIC_MODEL_HEADER
          value: "outlink-topic-model-predictor.articletopic-outlink.svc.cluster.local"
        - name: USE_CACHE
          value: "True"
        - name: CASSANDRA_DATACENTER
          value: "codfw"
        - name: CASSANDRA_SERVERS
          value: "cassandra-dev2001-a.codfw.wmnet;cassandra-dev2001-b.codfw.wmnet;cassandra-dev2002-a.codfw.wmnet;cassandra-dev2002-b.codfw.wmnet;cassandra-dev2003-a.codfw.wmnet;cassandra-dev2003-b.codfw.wmnet"
        - name: CASSANDRA_KEYSPACE
          value: "ml_cache"
        - name: CASSANDRA_TABLE
          value: "page_paragraph_tone_scores"
        - name: CASSANDRA_USER
          valueFrom:
            secretKeyRef:
              name: revise-tone-task-generator-cassandra-credentials
              key: CASSANDRA_USER
        - name: CASSANDRA_PASSWORD
          valueFrom:
            secretKeyRef:
              name: revise-tone-task-generator-cassandra-credentials
              key: CASSANDRA_PASSWORD
      container:
        resources:
          limits:
            cpu: "4"
            memory: 8Gi
          requests:
            cpu: "4"
            memory: 8Gi


external_services_app_label_selector: app-wmf
external_services:
  cassandra:
  - ml-cache-a-eqiad
  - ml-cache-a-codfw
  - cassandra-dev-a-codfw
  - cassandra-dev-b-codfw
  - analytics-query-service-storage-a-codfw
  - analytics-query-service-storage-b-codfw
  - analytics-query-service-storage-a-eqiad
  - analytics-query-service-storage-b-eqiad
