inference:
  predictor:
    config:
      # These settings are applied at the pod level,
      # that includes all containers not explicitly defined
      # in the isvc (like the istio-{validation,proxy}).
      # The knative-serving ones seem to need
      # an extra level of settings in the knative's control-plane
      # due to how the various pods/revisions are handled
      # (dynamically etc..).
      # More info https://phabricator.wikimedia.org/T369493
      securityContext:
        seccompProfile:
          type: RuntimeDefault

inference_services:
  revertrisk-wikidata:
    predictor:
      image: "machinelearning-liftwing-inference-services-revertrisk-wikidata"
      image_version: "2024-06-27-100336-publish"
      custom_env:
        - name: MODEL_NAME
          value: "revertrisk-wikidata"
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/experimental/revertrisk-wikidata/20230512162400/"
        - name: WIKI_URL
          value: "http://mw-api-int-ro.discovery.wmnet:4680"
      container:
        resources:
          limits:
            cpu: "2"
            memory: 4Gi
          requests:
            cpu: "2"
            memory: 4Gi

  articlequality:
    predictor:
      image: "machinelearning-liftwing-inference-services-articlequality"
      image_version: "2025-04-28-104618-publish"
      custom_env:
        - name: MODEL_NAME
          value: "articlequality"
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/articlequality/language-agnostic/20250425125943/"
        - name: FORCE_HTTP
          value: "True"
      container:
        resources:
          limits:
            memory: 1Gi
          requests:
            memory: 1Gi

  reference-need:
    annotations:
      autoscaling.knative.dev/target: "3"
    predictor:
      config:
        minReplicas: 1
        maxReplicas: 1
      image: "machinelearning-liftwing-inference-services-reference-quality"
      image_version: "2025-03-19-145037-publish"
      custom_env:
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/reference-quality/20250127142109/"
        - name: FORCE_HTTP
          value: "True"
        - name: MODEL_TO_DEPLOY
          value: "reference-need"
        - name: BATCH_SIZE
          value: "16"
          # We set number of threads for each worker model to (num_cpus/2)-1 to leave CPU capacity to the main event loop
        - name: NUM_THREADS
          value: "7"
        - name: NUM_OF_WORKERS
          value: "2"
      container:
        resources:
          limits:
            cpu: "16"
            memory: 4Gi
          requests:
            cpu: "16"
            memory: 4Gi

  reference-risk:
    annotations:
      autoscaling.knative.dev/target: "3"
    predictor:
      config:
        minReplicas: 1
        maxReplicas: 8
      image: "machinelearning-liftwing-inference-services-reference-quality"
      image_version: "2025-03-11-113532-publish"
      custom_env:
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/reference-quality/20250127142109/"
        - name: FORCE_HTTP
          value: "True"
        - name: MODEL_TO_DEPLOY
          value: "reference-risk"
      container:
        resources:
          limits:
            cpu: "4"
            memory: 3Gi
          requests:
            cpu: "4"
            memory: 3Gi


  article-country:
    predictor:
      image: "machinelearning-liftwing-inference-services-article-country"
      image_version: "2024-12-09-142136-publish"
      custom_env:
        - name: MODEL_NAME
          value: "article-country"
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/article-country/20240901015102/"
        - name: FORCE_HTTP
          value: "True"
      container:
        resources:
          limits:
            cpu: "2"
            memory: 2Gi
          requests:
            cpu: "2"
            memory: 2Gi

  edit-check:
    predictor:
      batcher:
        maxBatchSize: 32
        maxLatency: 20
      config:
        maxReplicas: 1
      image: "machinelearning-liftwing-inference-services-edit-check"
      image_version: "2025-04-15-093418-publish"
      custom_env:
        - name: MODEL_NAME
          value: "edit-check-staging"
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/edit-check/peacock/"
        - name: MODEL_VERSION
          value: "v1"
      container:
        resources:
          limits:
            cpu: "4"
            memory: 8Gi
            amd.com/gpu: "1"
          requests:
            cpu: "4"
            memory: 8Gi
            amd.com/gpu: "1"

external_services_app_label_selector: app-wmf
external_services:
  cassandra:
  - ml-cache-a-eqiad
  - ml-cache-a-codfw
