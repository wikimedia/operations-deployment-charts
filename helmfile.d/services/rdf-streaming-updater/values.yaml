app:
  config_files:
    rdf-streaming-updater-config.properties:
      checkpoint_interval: 30000
      checkpoint_timeout: 900000
      output_mutation_schema_version: v2
      page_change_stream: mediawiki.page_change.v1
      page_change_content_models: wikibase-item,wikibase-property,wikibase-lexeme
      use_event_streams_api: true
      stream_config_uri: https://meta.wikimedia.org/w/api.php
      schema_base_uris: https://schema.wikimedia.org/repositories/primary/jsonschema,https://schema.wikimedia.org/repositories/secondary/jsonschema
      generate_diff_timeout: -1
      http_routes: test-commons.wikimedia.org=localhost:6500,commons.wikimedia.org=localhost:6500,test.wikidata.org=localhost:6500,www.wikidata.org=localhost:6500,meta.wikimedia.org=localhost:6500,schema.wikimedia.org=localhost:6023
      input_idleness: 60000
      mediawiki_max_concurrent_requests: 12
      output_topic_partition: 0
      page_delete_topic: mediawiki.page-delete
      page_undelete_topic: mediawiki.page-undelete
      parallelism: 3
      produce_side_outputs: true
      restart_failures_rate_delay: 60000
      restart_failures_rate_interval: 180000
      restart_failures_rate_max_per_interval: 10
      rev_create_topic: mediawiki.revision-create
      suppressed_delete_topic: mediawiki.page-suppress
      topic_prefixes: eqiad.,codfw.
      wikibase_repo_thread_pool_size: 30
      kafka_producer_config.security.protocol: SSL
      kafka_producer_config.client.dns.lookup: use_all_dns_ips
      kafka_consumer_config.security.protocol: SSL
      kafka_consumer_config.client.dns.lookup: use_all_dns_ips
  image: docker-registry.discovery.wmnet/repos/wikidata-platform/flink-rdf-streaming-updater
  version: flink-1.20.2-rdf-0.3.162-scala-2.12.21
  flinkVersion: v1_20
  env:
    # disable JEMALLOC for testing
    - name: "DISABLE_JEMALLOC"
      value: "true"
  job:
    allowNonRestoredState: true # required because we are moving away from deprecated kafka connectors T326914
    args: [ "/srv/app/conf/rdf-streaming-updater-config.properties" ]
    entryClass: org.wikidata.query.rdf.updater.UpdaterJob
  # Savepoint TBD, will fill in during operations work
    jarURI: local:///opt/flink/streaming-updater-producer.jar
    upgradeMode: savepoint
  jobManager:
    replicas: 1
    resource:
      memory: 1600m

  taskManager:
    replicas: 3
    resource:
      memory: 4Gb
      cpu: 4

  flinkConfiguration:
    "taskmanager.numberOfTaskSlots": "1"
    "kubernetes.operator.savepoint.format.type": NATIVE
    "kubernetes.operator.savepoint.history.max.count": "5"
    "kubernetes.operator.savepoint.trigger.grace-period": 20sec
    "kubernetes.operator.cluster.health-check.checkpoint-progress.window": 10min
    "taskmanager.memory.jvm-overhead.max": "2Gb"
    "taskmanager.memory.jvm-overhead.fraction": "0.45"
    "taskmanager.memory.managed.consumer-weights": "STATE_BACKEND:100"
    "s3.access-key": wdqs:savepoints
    # We can put this config in values-main.yaml because it uses discovery endpoint,
    # which will be resolved correctly depending on which DC we are deploying to.
    "s3.endpoint": thanos-swift.discovery.wmnet
    "s3.path.style.access": "true"
    "s3.socket-timeout": "30s"
    "state.backend.rocksdb.metrics.estimate-num-keys": "true"
    "state.backend.rocksdb.metrics.estimate-live-data-size": "true"
    "state.backend.rocksdb.metrics.cur-size-active-mem-table": "true"
    "state.backend.rocksdb.metrics.size-all-mem-tables": "true"
discovery:
  listeners:
    - mw-api-int-async-ro
    # TODO: consider packaging the schemas in the image directly instead of relying on the service at runtime
    - schema
    - thanos-swift

flink:
  object_store:
    swift_access_key: wdqs:savepoints
    swift_secret_key: secret
    # We can put this config in values-main.yaml because it uses discovery endpoint,
    # which will be resolved correctly depending on which DC we are deploying to.
    swift_cluster: thanos-swift.discovery.wmnet

# Enable the service mesh.
mesh:
  enabled: true

# Enable egress.  Specific egress policies should either be added in
# environment/k8s cluster specific networkpolicy.egress.dst_nets,
# or automatically configured via discovery.listeners and/or kafka.allowed_clusters,
networkpolicy:
  egress:
    enabled: true
