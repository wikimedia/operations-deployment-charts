main_app:
  version: 2026-02-20-232022-production

resources:
  replicas: 2

service:
  deployment: production

mesh:
  enabled: true
  public_port: 4011
  certmanager:
    extraFQDNs:
      - toolhub.wikimedia.org

mariadb:
  egress:
    sections: ["m5"]

networkpolicy:
  egress:
    enabled: true
    dst_nets:
      # -- search.svc.{eqiad,codfw}.wmnet is covered by envoy
      # -- url-downloader is covered by default egress rules
      # -- T291447: allow direct access to text-lb nodes
      - cidr: 208.80.154.224/32  # text-lb.eqiad.wikimedia.org
        ports:
          - protocol: TCP
            port: 443
      - cidr: 2620:0:861:ed1a::1/128  # text-lb.eqiad.wikimedia.org
        ports:
          - protocol: TCP
            port: 443
      - cidr: 208.80.153.224/32  # text-lb.codfw.wikimedia.org
        ports:
          - protocol: TCP
            port: 443
      - cidr: 2620:0:860:ed1a::1/128  # text-lb.codfw.wikimedia.org
        ports:
          - protocol: TCP
            port: 443

discovery:
  listeners:
    - search-chi-codfw
    - search-chi-eqiad

mcrouter:
  enabled: true
  zone: eqiad
  route_prefix: eqiad/toolhub
  routes:
    - route: /eqiad/toolhub
      type: standalone
      failover: true
      # Pool config from /etc/helmfile-defaults/mediawiki/mcrouter_pools.yaml
      pool: eqiad-servers

config:
  public:
    URLLIB3_DISABLE_WARNINGS: True  # T292025
    REQUIRE_HTTPS: True
    DB_HOST: m5-master.eqiad.wmnet
    CACHE_BACKEND: django_prometheus.cache.backends.memcached.MemcachedCache
    CACHE_LOCATION: mcrouter:11213  # Sidecar container
    ES_HOSTS: localhost:6102  # Envoy proxy to search-chi-eqiad
    ES_INDEX_REPLICAS: 2
    WIKIMEDIA_OAUTH2_KEY: 89c2e5fbbbbe57687b8757c46cb74901
    http_proxy: http://url-downloader.eqiad.wikimedia.org:8080
    https_proxy: http://url-downloader.eqiad.wikimedia.org:8080
    no_proxy: localhost,mediawiki.org,wikibooks.org,wikidata.org,wikimedia.org,wikinews.org,wikipedia.org,wikiquote.org,wikisource.org,wikiversity.org,wikivoyage.org,wiktionary.org

crawler:
  concurrencyPolicy: Replace
  successfulJobsHistoryLimit: 0
  failedJobsHistoryLimit: 1
