# Default values.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
docker:
  registry: docker-registry.wikimedia.org
  pull_policy: IfNotPresent

config:
  private: {}

  hadoop:
    hdfs:
      dfs.block.access.token.enable: 'true'
      dfs.data.transfer.protection: privacy
      dfs.encrypt.data.transfer: 'true'
      dfs.encrypt.data.transfer.cipher.key.bitlength: '128'
      dfs.encrypt.data.transfer.cipher.suites: AES/CTR/NoPadding
      dfs.ha.automatic-failover.enabled: 'true'
      dfs.internal.nameservices: override_me
      dfs.nameservices: override_me
      dfs.webhdfs.enabled: 'false'
    core:
      fs.defaultFS: override_me
      fs.permissions.umask-mode: '027'
      fs.trash.checkpoint.interval: '1440'
      fs.trash.interval: '43200'
      hadoop.proxyuser.mapred.groups: '*'
      hadoop.proxyuser.mapred.hosts: '*'
      hadoop.rpc.protection: privacy
      hadoop.security.authentication: kerberos
      hadoop.ssl.enabled.protocols: TLSv1.2
      io.file.buffer.size: '131072'
    hive:
      hive.cli.print.current.db: 'true'
      hive.cli.print.header: 'true'
      hive.default.fileformat: parquet
      hive.error.on.empty.partition: 'true'
      hive.exec.parallel: 'true'
      hive.exec.parallel.thread.number: '8'
      hive.exec.stagingdir: /tmp/hive-staging
      hive.exec.submit.local.task.via.child: 'false'
      hive.mapred.mode: strict
      hive.metastore.disallow.incompatible.col.type.changes: 'false'
      hive.metastore.execute.setugi: 'true'
      hive.metastore.kerberos.principal: override_me
      hive.metastore.sasl.enabled: 'true'
      hive.metastore.uris: override_me
      hive.resultset.use.unique.column.names: 'false'
      hive.server2.authentication: KERBEROS
      hive.start.cleanup.scratchdir: 'true'
      hive.stats.autogather: 'false'
      hive.support.concurrency: 'false'
      hive.variable.substitute.depth: '10000'
      parquet.compression: SNAPPY
  spark:
    spark.master: k8s://https://kubernetes.default.svc.cluster.local:443
    spark.kubernetes.container.image: docker-registry.discovery.wmnet/repos/data-engineering/spark:latest
    spark.authenticate: true
    spark.blockManager.port: 7079
    spark.driver.port: 7078
    spark.dynamicAllocation.cachedExecutorIdleTimeout: 3600s
    spark.dynamicAllocation.enabled: true
    spark.dynamicAllocation.executorIdleTimeout: 60s
    spark.eventLog.compress: true
    spark.eventLog.dir: hdfs:///var/log/spark
    spark.eventLog.enabled: true
    spark.kubernetes.authenticate.driver.serviceAccountName: spark
    spark.network.crypto.enabled: true
    spark.network.crypto.keyFactoryAlgorithm: PBKDF2WithHmacSHA256
    spark.network.crypto.keyLength: 256
    spark.network.crypto.saslFallback: false
    spark.sql.catalog.spark_catalog: org.apache.iceberg.spark.SparkSessionCatalog
    spark.sql.catalog.spark_catalog.type: hive
    spark.sql.catalogImplementation: hive
    spark.sql.extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
    spark.sql.files.maxPartitionBytes: '268435456'
    spark.sql.warehouse.dir: hdfs:///user/hive/warehouse
  kyuubi:
    kyuubi.authentication: KERBEROS
    kyuubi.engine.share.level: CONNECTION
    kyuubi.engine.single.spark.session: true
    kyuubi.engine.session.user.impersonation: false
    kyuubi.kinit.principal: override_me
    kyuubi.kinit.keytab: override_me
    kyuubi.server.auth.kerberos.principal: override_me
    kyuubi.server.auth.kerberos.keytab: override_me
    ## TODO - Use a production JDBC URL to enable high-availability for Kyuubi. See the following:
    ## https://kyuubi.readthedocs.io/en/master/configuration/settings.html#metadata
    ## https://kyuubi.readthedocs.io/en/master/deployment/high_availability_guide.html#production-mode
    kyuubi.metadata.store.jdbc.url: jdbc:sqlite:/tmp/kyuubi_state_store.db
    kyuubi.ha.addresses: ~
    kyuubi.zookeeper.embedded.data.log.dir: /tmp
    kyuubi.zookeeper.embedded.data.dir: /tmp


spark:
  toolbox:
    replicas: 1
    affinity: {}
    resources:
      requests:
        cpu: 500m
        memory: 2Gi
      limits:
        cpu: 2000m
        memory: 4Gi
  volumes:
    - name: spark-pod-templates
      configMap:
        name: spark-pod-templates
    - name: spark-configuration
      configMap:
        name: spark-configuration
  volumemounts:
    - mountPath: /etc/spark/pod_templates
      name: spark-pod-templates
    - mountPath: /etc/spark/conf/spark-defaults.conf
      subPath: spark-defaults.conf
      name: spark-configuration
    - mountPath: /etc/spark/conf/hive-site.xml
      subPath: hive-site.xml
      name: hadoop-configuration

hadoop:
  volumes:
    - name: hadoop-configuration
      configMap:
        name: hadoop-configuration
  volumemounts:
  - mountPath: /etc/hadoop/conf/core-site.xml
    subPath: core-site.xml
    name: hadoop-configuration
  - mountPath: /etc/hadoop/conf/hdfs-site.xml
    subPath: hdfs-site.xml
    name: hadoop-configuration
  - mountPath: /etc/hadoop/conf/hive-site.xml
    subPath: hive-site.xml
    name: hadoop-configuration

kerberos:
  keytabs: ~ # Overridden in the private repo.
  servers: ['override_me'] # Overridden in the /etc/helmfile-defaults/general-{{ .Environment.Name }}.yaml file.
  admin: 'override_me' # Overridden in the /etc/helmfile-defaults/general-{{ .Environment.Name }}.yaml file.
  volumes:
    - name: kerberos-client-configuration
      configMap:
        name: kerberos-client-configuration
    - name: kerberos-keytabs
      secret:
        secretName: kerberos-keytabs
  volumemounts:
    - mountPath: /etc/krb5.conf
      subPath: krb5.conf
      name: kerberos-client-configuration
    - mountPath: /etc/security/keytabs
      name: kerberos-keytabs

kyuubi:
  replicas: 1
  image: docker-registry.discovery.wmnet/repos/data-engineering/spark/kyuubi:latest
  affinity: {}
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
    limits:
      cpu: 2000m
      memory: 4Gi
  volumes:
    - name: kyuubi-configuration
      configMap:
        name: kyuubi-configuration
  volumemounts:
    - mountPath: /etc/kyuubi/conf
      name: kyuubi-configuration

  service:
    headless:
      annotations: {}

  server:
    # Thrift Binary protocol (HiveServer2 compatible)
    thriftBinary:
      enabled: true
      port: 10009
      service:
        type: ClusterIP
        port: "{{ .Values.kyuubi.server.thriftBinary.port }}"
        nodePort: ~
        annotations: {}
        sessionAffinity: ~
        sessionAffinityConfig: {}

    # Thrift HTTP protocol (HiveServer2 compatible)
    thriftHttp:
      enabled: false
      port: 10010
      service:
        type: ClusterIP
        port: "{{ .Values.kyuubi.server.thriftHttp.port }}"
        nodePort: ~
        annotations: {}
        sessionAffinity: ~
        sessionAffinityConfig: {}

    # REST API protocol (experimental)
    rest:
      enabled: false
      port: 10099
      service:
        type: ClusterIP
        port: "{{ .Values.kyuubi.server.rest.port }}"
        nodePort: ~
        annotations: {}
        sessionAffinity: ~
        sessionAffinityConfig: {}

    # MySQL compatible text protocol (experimental)
    mysql:
      enabled: false
      port: 3309
      service:
        type: ClusterIP
        port: "{{ .Values.kyuubi.server.mysql.port }}"
        nodePort: ~
        annotations: {}
        sessionAffinity: ~
        sessionAffinityConfig: {}

  # Liveness probe
  livenessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 2
    failureThreshold: 10
    successThreshold: 1

  # Readiness probe
  readinessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 2
    failureThreshold: 10
    successThreshold: 1

  # Metrics configuration
  metrics:
    # Enable metrics system, used for 'kyuubi.metrics.enabled' property
    enabled: true
    # A comma-separated list of metrics reporters, used for 'kyuubi.metrics.reporters' property
    reporters: PROMETHEUS
    # Prometheus port, used for 'kyuubi.metrics.prometheus.port' property
    prometheusPort: 10019

monitoring:
  # If enabled is true, monitoring annotations will be added to the deployment.
  enabled: false

networkpolicy:
  egress:
    enabled: false

# Add here the list of kafka-clusters (by name) that the service will need to reach.
kafka:
  allowed_clusters: []

# See https://wikitech.wikimedia.org/wiki/Kubernetes/Deployment_Charts#Enabling_egress_to_services_external_to_Kubernetes
# for the list of supported services
external_services: {}

# This allows for serviceaccounts in other namespaces to br granted the rights to manage pods
# within this namespace.
orchestrator:
  service_accounts: {}
# orchestrator:
#   service_accounts:
#     airflow-dummy: [airflow]
#     airflow-dev: [airflow-dev]

# Definition of extra IP: hostnames to be injected in /etc/hosts
# Typically, we use this to force the reverse DNS resolution of an an-master host
# IP to its an-masterxxxx.eqiad.wmnet name and to avoid the coredns service ip reverse
# DNS, to validate Kerberos identity.
host_aliases: {}
