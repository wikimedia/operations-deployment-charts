# Default values.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
docker:
  registry: docker-registry.wikimedia.org
  pull_policy: IfNotPresent

kyuubi:
  replicas: 1
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
    limits:
      cpu: 2000m
      memory: 4Gi

toolbox:
  replicas: 1
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
    limits:
      cpu: 2000m
      memory: 4Gi

config:
  private: {}

  hadoop:
    hdfs:
      dfs.block.access.token.enable: 'true'
      dfs.data.transfer.protection: privacy
      dfs.encrypt.data.transfer: 'true'
      dfs.encrypt.data.transfer.cipher.key.bitlength: '128'
      dfs.encrypt.data.transfer.cipher.suites: AES/CTR/NoPadding
      dfs.ha.automatic-failover.enabled: 'true'
      dfs.internal.nameservices: override_me
      dfs.nameservices: override_me
      dfs.webhdfs.enabled: 'false'
    core:
      fs.defaultFS: override_me
      fs.permissions.umask-mode: '027'
      fs.trash.checkpoint.interval: '1440'
      fs.trash.interval: '43200'
      hadoop.proxyuser.mapred.groups: '*'
      hadoop.proxyuser.mapred.hosts: '*'
      hadoop.rpc.protection: privacy
      hadoop.security.authentication: kerberos
      hadoop.ssl.enabled.protocols: TLSv1.2
      io.file.buffer.size: '131072'
    hive:
      hive.cli.print.current.db: 'true'
      hive.cli.print.header: 'true'
      hive.default.fileformat: parquet
      hive.error.on.empty.partition: 'true'
      hive.exec.parallel: 'true'
      hive.exec.parallel.thread.number: '8'
      hive.exec.stagingdir: /tmp/hive-staging
      hive.exec.submit.local.task.via.child: 'false'
      hive.mapred.mode: strict
      hive.metastore.disallow.incompatible.col.type.changes: 'false'
      hive.metastore.execute.setugi: 'true'
      hive.metastore.kerberos.principal: override_me
      hive.metastore.sasl.enabled: 'true'
      hive.metastore.uris: override_me
      hive.resultset.use.unique.column.names: 'false'
      hive.server2.authentication: KERBEROS
      hive.start.cleanup.scratchdir: 'true'
      hive.stats.autogather: 'false'
      hive.support.concurrency: 'false'
      hive.variable.substitute.depth: '10000'
      parquet.compression: SNAPPY
  spark:
    spark.master: k8s://https://kubernetes.default.svc.cluster.local:443
    spark.kubernetes.container.image: docker-registry.discovery.wmnet/repos/data-engineering/spark:latest
    spark.authenticate: true
    spark.blockManager.port: 7079
    spark.driver.port: 7078
    spark.dynamicAllocation.cachedExecutorIdleTimeout: 3600s
    spark.dynamicAllocation.enabled: true
    spark.dynamicAllocation.executorIdleTimeout: 60s
    spark.eventLog.compress: true
    spark.eventLog.dir: hdfs:///var/log/spark
    spark.eventLog.enabled: true
    spark.kubernetes.authenticate.driver.serviceAccountName: spark
    spark.network.crypto.enabled: true
    spark.network.crypto.keyFactoryAlgorithm: PBKDF2WithHmacSHA256
    spark.network.crypto.keyLength: 256
    spark.network.crypto.saslFallback: false
    spark.sql.catalog.spark_catalog: org.apache.iceberg.spark.SparkSessionCatalog
    spark.sql.catalog.spark_catalog.type: hive
    spark.sql.catalogImplementation: hive
    spark.sql.extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
    spark.sql.files.maxPartitionBytes: '268435456'
    spark.sql.warehouse.dir: hdfs:///user/hive/warehouse
  kyuubi:
    image: docker-registry.discovery.wmnet/repos/data-engineering/spark/kyuubi:latest

volumes:
  - name: spark-pod-templates
    configMap:
      name: spark-pod-templates
  - name: spark-configuration
    configMap:
      name: spark-configuration
  - name: hadoop-configuration
    configMap:
      name: hadoop-configuration
  - name: kerberos-client-config
    configMap:
      name: kerberos-client-config
  - name: kerberos-keytabs
    secret:
      secretName: kerberos-keytabs

volumemounts:
  - mountPath: /etc/spark/pod_templates
    name: spark-pod-templates
  - mountPath: /etc/spark/conf/spark-defaults.conf
    subPath: spark-defaults.conf
    name: spark-configuration
  - mountPath: /etc/spark/conf/hive-site.xml
    subPath: hive-site.xml
    name: hadoop-configuration
  - mountPath: /etc/hadoop/conf/core-site.xml
    subPath: core-site.xml
    name: hadoop-configuration
  - mountPath: /etc/hadoop/conf/hdfs-site.xml
    subPath: hdfs-site.xml
    name: hadoop-configuration
  - mountPath: /etc/hadoop/conf/hive-site.xml
    subPath: hive-site.xml
    name: hadoop-configuration
  - mountPath: /etc/krb5.conf
    subPath: krb5.conf
    name: kerberos-client-config
  - mountPath: /etc/security/keytabs
    name: kerberos-keytabs


monitoring:
  # If enabled is true, monitoring annotations will be added to the deployment.
  enabled: false

networkpolicy:
  egress:
    enabled: false

# Add here the list of kafka-clusters (by name) that the service will need to reach.
kafka:
  allowed_clusters: []

# Optional affinity settings
affinity: {}
#  affinity:
#    nodeAffinity:
#      requiredDuringSchedulingIgnoredDuringExecution:
#        nodeSelectorTerms:
#          - matchExpressions:
#              - key: some-key
#                operator: In
#                values:
#                  - some-value
#  nodeSelector:
#    node.kubernetes.io/some-key: some-value

# The set of external services to allow egress to
# Example:
# kafka:
# - main-codfw
# - main-eqiad
# presto:
# - analytics
#
# See https://wikitech.wikimedia.org/wiki/Kubernetes/Deployment_Charts#Enabling_egress_to_services_external_to_Kubernetes
# for the list of supported services
external_services: {}

# This allows for serviceaccounts in other namespaces to br granted the rights to manage pods
# within this namespace.
orchestrator:
  service_accounts: {}
# orchestrator:
#   service_accounts:
#     airflow-dummy: [airflow]
#     airflow-dev: [airflow-dev]

# Definition of extra IP: hostnames to be injected in /etc/hosts
# Typically, we use this to force the reverse DNS resolution of an an-master host
# IP to its an-masterxxxx.eqiad.wmnet name and to avoid the coredns service ip reverse
# DNS, to validate Kerberos identity.
host_aliases: {}

kerberos:
  keytabs: ~ # Overridden in the private repo.
  servers: ['override_me'] # Overridden in the /etc/helmfile-defaults/general-{{ .Environment.Name }}.yaml file.
  admin: 'override_me' # Overridden in the /etc/helmfile-defaults/general-{{ .Environment.Name }}.yaml file.