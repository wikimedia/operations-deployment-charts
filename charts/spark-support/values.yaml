# Default values.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

_vars:
  hadoop_conf_dir: &hadoop_conf_dir /etc/hadoop/conf

docker:
  registry: docker-registry.wikimedia.org
  pull_policy: IfNotPresent
resources:
  replicas: 1

app:
  port: 8080 # Not required

config:
  public:
    "HADOOP_CONF_DIR": *hadoop_conf_dir
    "HIVE_CONF_DIR": *hadoop_conf_dir
  private: {}

  hadoop:
    hdfs:
      dfs.ha.automatic-failover.enabled: 'true'
      dfs.nameservices: override_me
      dfs.internal.nameservices: override_me
      dfs.webhdfs.enabled: 'false'
      dfs.block.access.token.enable: 'true'
      dfs.data.transfer.protection: privacy
      dfs.encrypt.data.transfer: 'true'
      dfs.encrypt.data.transfer.cipher.key.bitlength: '128'
      dfs.encrypt.data.transfer.cipher.suites: AES/CTR/NoPadding
    core:
      fs.defaultFS: override_me
      ha.zookeeper.quorum: override_me
      io.file.buffer.size: '131072'
      hadoop.proxyuser.mapred.hosts: '*'
      hadoop.proxyuser.mapred.groups: '*'
      fs.trash.checkpoint.interval: '1440'
      fs.trash.interval: '43200'
      fs.permissions.umask-mode: '027'
      hadoop.rpc.protection: privacy
      hadoop.security.authentication: kerberos
      hadoop.ssl.enabled.protocols: TLSv1.2
    yarn:
      spark.authenticate: 'true'
      spark.network.crypto.enabled: 'true'
      yarn.application.classpath: $HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,$HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,$HADOOP_MAPRED_HOME/*,$HADOOP_MAPRED_HOME/lib/*,$HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*
      yarn.log-aggregation-enable: 'true'
      yarn.log-aggregation.retain-check-interval-seconds: '86400'
      yarn.log-aggregation.retain-seconds: '5184000'
      yarn.resourcemanager.ha.rm-ids: override_me
      yarn.resourcemanager.principal: yarn/_HOST@WIKIMEDIA
      yarn.scheduler.maximum-allocation-mb: '49152'
      yarn.scheduler.maximum-allocation-vcores: '32'
      yarn.scheduler.minimum-allocation-mb: '1'
      yarn.scheduler.minimum-allocation-vcores: '1'
    hive:
      hive.metastore.uris: override_me
      hive.metastore.kerberos.keytab.file: override_me
      hive.metastore.kerberos.principal: override_me
      hive.metastore.sasl.enabled: 'true'
      hive.support.concurrency: 'false'
      hive.metastore.disallow.incompatible.col.type.changes: 'false'
      hive.metastore.execute.setugi: 'true'
      hive.cli.print.current.db: 'true'
      hive.cli.print.header: 'true'
      hive.mapred.mode: strict
      hive.start.cleanup.scratchdir: 'true'
      hive.exec.stagingdir: /tmp/hive-staging
      hive.error.on.empty.partition: 'true'
      hive.exec.parallel: 'true'
      hive.exec.parallel.thread.number: '8'
      hive.stats.autogather: 'false'
      hive.variable.substitute.depth: '10000'
      hive.default.fileformat: parquet
      parquet.compression: SNAPPY
      hive.server2.builtin.udf.blacklist: xpath,xpath_string,xpath_boolean,xpath_number,xpath_double,xpath_float,xpath_long,xpath_int,xpath_short
      hive.resultset.use.unique.column.names: 'false'
      hive.exec.submit.local.task.via.child: 'false'
      hive.server2.authentication: KERBEROS
      hive.server2.logging.operation.enabled: 'true'
  spark:
    spark:
      spark.dynamicAllocation.enabled: true
      spark.dynamicAllocation.executorIdleTimeout: 60s
      spark.dynamicAllocation.cachedExecutorIdleTimeout: 3600s
      spark.sql.catalogImplementation: hive
      spark.sql.extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
      spark.sql.catalog.spark_catalog: org.apache.iceberg.spark.SparkSessionCatalog
      spark.sql.catalog.spark_catalog.type: hive
      spark.sql.files.maxPartitionBytes: '268435456'
      spark.sql.warehouse.dir: hdfs:///user/hive/warehouse
      spark.authenticate: true
      spark.network.crypto.enabled: true
      spark.network.crypto.keyFactoryAlgorithm: PBKDF2WithHmacSHA256
      spark.network.crypto.keyLength: 256
      spark.network.crypto.saslFallback: false
      spark.eventLog.enabled: true
      spark.eventLog.dir: hdfs:///var/log/spark
      spark.eventLog.compress: true
      spark.kubernetes.authenticate.driver.serviceAccountName: spark

monitoring:
  # If enabled is true, monitoring annotations will be added to the deployment.
  enabled: false

networkpolicy:
  egress:
    enabled: false

# Add here the list of kafka-clusters (by name) that the service will need to reach.
kafka:
  allowed_clusters: []

# Optional affinity settings
affinity: {}
#  affinity:
#    nodeAffinity:
#      requiredDuringSchedulingIgnoredDuringExecution:
#        nodeSelectorTerms:
#          - matchExpressions:
#              - key: some-key
#                operator: In
#                values:
#                  - some-value
#  nodeSelector:
#    node.kubernetes.io/some-key: some-value

# The set of external services to allow egress to
# Example:
# kafka:
# - main-codfw
# - main-eqiad
# presto:
# - analytics
#
# See https://wikitech.wikimedia.org/wiki/Kubernetes/Deployment_Charts#Enabling_egress_to_services_external_to_Kubernetes
# for the list of supported services
external_services: {}

# This allows for serviceaccounts in other namespaces to br granted the rights to manage pods
# within this namespace.
orchestrator:
  service_accounts: {}
# orchestrator:
#   service_accounts:
#     airflow-dummy: [airflow]
#     airflow-dev: [airflow-dev]

# Definition of extra IP: hostnames to be injected in /etc/hosts
# Typically, we use this to force the reverse DNS resolution of an an-master host
# IP to its an-masterxxxx.eqiad.wmnet name and to avoid the coredns service ip reverse
# DNS, to validate Kerberos identity.
host_aliases: {}

kerberos:
  keytabs: ~ # Overridden in the private repo.
  servers: ['override_me'] # Overridden in the /etc/helmfile-defaults/general-{{ .Environment.Name }}.yaml file.
  admin: 'override_me' # Overridden in the /etc/helmfile-defaults/general-{{ .Environment.Name }}.yaml file.