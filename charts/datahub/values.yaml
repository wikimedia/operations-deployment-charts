# Default values for datahub.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
helm_scaffold_version: 0.4 # This can be useful when backporting fixes.
docker:
  registry: docker-registry.wikimedia.org
  pull_policy: IfNotPresent

# Start of batch job configuration. These values are used by three setup jobs
# that setup elasticsearch (aka opensearch), kafka, and mysql (aka mariadb).
# The jobs run in the context of the parent chart
elasticsearchSetupJob:
  enabled: false
  image:
    repository: repos/data-engineering/datahub/elasticsearch-setup
    tag: latest

kafkaSetupJob:
  enabled: false
  image:
    repository: repos/data-engineering/datahub/kafka-setup
    tag: latest

mysqlSetupJob:
  enabled: false
  image:
    repository: repos/data-engineering/datahub/mysql-setup
    tag: latest

datahubUpgrade:
  enabled: true
  image:
    repository: repos/data-engineering/datahub/upgrade
    tag: latest
  batchSize: 1000
  batchDelayMs: 100
  noCodeDataMigration:
    sqlDbType: "MYSQL"
    resources:
      limits:
        cpu: 500m
        memory: 512Mi
      requests:
        cpu: 300m
        memory: 256Mi
  podSecurityContext: {}
  securityContext: {}
  podAnnotations: {}
  extraSidecars: []
  cleanupJob:
    resources:
      limits:
        cpu: 500m
        memory: 512Mi
      requests:
        cpu: 300m
        memory: 256Mi
    extraSidecars: []
  restoreIndices:
    resources:
      limits:
        cpu: 500m
        memory: 512Mi
      requests:
        cpu: 300m
        memory: 256Mi
    extraSidecars: []
## Runs system update processes
## Includes: Elasticsearch Indices Creation/Reindex (See global.elasticsearch.index for additional configuration)
datahubSystemUpdate:
  image:
    repository: repos/data-engineering/datahub/upgrade
    tag: latest
  podSecurityContext: {}
  securityContext: {}
  podAnnotations: {}
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 300m
      memory: 256Mi
  extraSidecars: []
# This networkpolicy applies to all three setup jobs (Elasticsearch, Kafka, MySQL) and
# the other batch jobs that run from this chart, but is not passed down to the subcharts.
networkpolicy:
  egress:
    enabled: false
# End of batch job configuration
mesh:
  enabled: false
# Start of subchart configuration
kafka: &kafka
  allowed_clusters: []

kafka_brokers: &kafka_brokers
  {}

config: &config
  public: {}
  private:
    datahub_encryption_key: ""    # This is used for at-rest encryption
    elasticsearch_password: ""    # This is used to encrypt the ES data in-flight
    mysql_password: ""            # This is the MySQL user account password
    token_service_signing_key: "" # This is used for GMS server authentication
    auth_oidc_client_secret: ""   # This is used for authentication to the IDP

datahub-frontend:
  auth:
    ldap:
      enabled: false
    oidc:
      enabled: true
      client_id: "datahub"
      base_url: "https://datahub.wikimedia.org/"
      discovery_uri: "https://idp.wikimedia.org/oidc/.well-known"
  kafka: *kafka
  kafka_brokers: *kafka_brokers
  config:
    <<: *config
    public:
      AUTH_NATIVE_ENABLED: false
  # The set of external services to allow egress to
  # Example:
  # kafka:
  # - main-codfw
  # - main-eqiad
  # presto:
  # - analytics
  #
  # See https://wikitech.wikimedia.org/wiki/Kubernetes/Deployment_Charts#Enabling_egress_to_services_external_to_Kubernetes
  # for the list of supported services
  external_services: {}

datahub-gms:
  kafka: *kafka
  kafka_brokers: *kafka_brokers
  config:
    <<: *config
    public:
      DATAHUB_TELEMETRY_ENABLED: false

datahub-mce-consumer:
  kafka: *kafka
  kafka_brokers: *kafka_brokers
  config:
    <<: *config

datahub-mae-consumer:
  kafka: *kafka
  kafka_brokers: *kafka_brokers
  config:
    <<: *config
# End of subchart configuration

# Start of global values. Values configured here will be made available to all
# dependent charts. Therefore this is the correct place to set values for use
# in the frontend, gms, mce-consumer, and mae-consumer pods.
#
# Note that changes made here should be replicated to the values.yaml files in
# each of the dependent charts. The reason for this is to ensure that
# 'helm validate' works on each dependent chart when validated individually.
#
global:
  graph_service_impl: elasticsearch
  datahub_analytics_enabled: true

  # This is used as a conditional to determine whether or not the mce and mae
  # consumer jobs are deployed as separate pods. If not, they run within the GMS.
  datahub_standalone_consumers_enabled: false

  podLabels:
    workload: datahub

  elasticsearch:
    host: dummy
    port: dummy
    skipcheck: "false"
    insecure: "true"
    ## The following section controls when and how reindexing of elasticsearch indices are performed
    index:
      ## Enable reindexing when mappings change based on the data model annotations
      enableMappingsReindex: true

      ## Enable reindexing when static index settings change.
      ## Dynamic settings which do not require reindexing are not affected
      ## Primarily this should be enabled when re-sharding is necessary for scaling/performance.
      enableSettingsReindex: true

      ## Index settings can be overridden for entity indices or other indices on an index by index basis
      ## Some index settings, such as # of shards, requires reindexing while others, i.e. replicas, do not
      ## Non-Entity indices do not require the prefix
      # settingsOverrides: '{"graph_service_v1":{"number_of_shards":"5"},"system_metadata_service_v1":{"number_of_shards":"5"}}'
      ## Entity indices do not require the prefix or suffix
      # entitySettingsOverrides: '{"dataset":{"number_of_shards":"10"}}'

      ## The amount of delay between indexing a document and having it returned in queries
      ## Increasing this value can improve performance when ingesting large amounts of data
      # refreshIntervalSeconds: 1

      ## The following options control settings for datahub-upgrade job when creating or reindexing indices
      upgrade:
        ## When reindexing is required, this option will clone the existing index as a backup
        ## The clone indices are not currently managed.
        cloneIndices: true

        ## Typically when reindexing the document counts between the original and destination indices should match.
        ## In some cases reindexing might not be able to proceed due to incompatibilities between a document in the
        ## orignal index and the new index's mappings. This document could be dropped and re-ingested or restored from
        ## the SQL database.
        ##
        ## This setting allows continuing if and only if the cloneIndices setting is also enabled which
        ## ensures a complete backup of the original index is preserved.
        allowDocCountMismatch: false

    ## Search related configuration
    search:
      ## Maximum terms in aggregations
      maxTermBucketSize: 20

      ## Configuration around exact matching for search
      exactMatch:
        ## if false will only apply weights, if true will exclude non-exact
        exclusive: false
        ## include prefix exact matches
        withPrefix: true
        ## boost multiplier when exact with case
        exactFactor: 2.0
        ## boost multiplier when exact prefix
        prefixFactor: 1.6
        ## stacked boost multiplier when case mismatch
        caseSensitivityFactor: 0.7
        ## enable exact match on structured search
        enableStructured: true

      ## Configuration for graph service dao
      graph:
        ## graph dao timeout seconds
        timeoutSeconds: 50
        ## graph dao batch size
        batchSize: 1000
        ## graph dao max result size
        maxResult: 10000

  kafka:
    bootstrap:
      server: ~
    schemaregistry:
    zookeeper:
      server: ~
    topics:
      metadata_change_event_name: "MetadataChangeEvent_v4"
      failed_metadata_change_event_name: "FailedMetadataChangeEvent_v4"
      metadata_audit_event_name: "MetadataAuditEvent_v4"
      datahub_usage_event_name: "DataHubUsageEvent_v1"
      metadata_change_proposal_topic_name: "MetadataChangeProposal_v1"
      failed_metadata_change_proposal_topic_name: "FailedMetadataChangeProposal_v1"
      metadata_change_log_versioned_topic_name: "MetadataChangeLog_Versioned_v1"
      metadata_change_log_timeseries_topic_name: "MetadataChangeLog_Timeseries_v1"
      platform_event_topic_name: "PlatformEvent_v1"
      datahub_upgrade_history_topic_name: "DataHubUpgradeHistory_v1"
    # partitions: 3
    # replicationFactor: 3
    schemaregistry:
      url: ~
      type: KAFKA

  sql:
    datasource:
      host: ~
      hostForMysqlClient: ~
      port: ~
      url: ~
      driver: "com.mysql.cj.jdbc.Driver"
      username: ~

  datahub:
    gms:
      port: "8080"
      useSSL: false

    monitoring:
      enablePrometheus: true

    play:
      mem:
        buffer:
          size: "100m"

    managed_ingestion:
      enabled: false

    metadata_service_authentication:
      enabled: false
      systemClientId: "__datahub_system"
    systemUpdate:
      enabled: true
    enable_retention: true
    cache:
      search:
        ## Enable general search caching
        enabled: false
        ## Configuration for the primary cache
        primary:
          ttlSeconds: 600
          maxSize: 10000
        ## Configuration for homepage cache
        homepage:
          entityCounts:
            ttlSeconds: 600
        ## Lineage specific caching options
        lineage:
          ## Enables in-memory cache for searchAcrossLineage query
          enabled: false
          ttlSeconds: 86400
          lightningThreshold: 300
    ## Enables always emitting a MCL even when no changes are detected. Used for Time Based Lineage when no changes occur.
    alwaysEmitChangeLog: false
    ## Enables diff mode for graph writes, uses a different code path that produces a diff from previous to next to write relationships instead of wholesale deleting edges and reading
    enableGraphDiffMode: true
    ## Values specific to the unified search and browse feature.
    search_and_browse:
      show_search_v2: true  # If on, show the new search filters experience as of v0.10.5
      show_browse_v2: true  # If on, show the new browse experience as of v0.10.5
      backfill_browse_v2: true  # If on, run the backfill upgrade job that generates default browse paths for relevant entities
# End of global values
