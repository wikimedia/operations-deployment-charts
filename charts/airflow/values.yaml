# Default values.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
app:
  image: &base-image repos/data-engineering/airflow
  version: &base-image-version latest # we use latest everywhere in the defaults.
  executor_pod_image: *base-image
  executor_pod_image_version: *base-image-version
  port: &airflow_frontend 8080 # airflow webUI
  # port: *airflow_frontend # port exposed as a Service, also used by service-checker.
  # Use command and args below to override the entrypoint. Type is arrays
  # Not necessary unless you want to change the entrypoint defined in the docker image
  # Example:
  # command: ["node"]
  # args: ["bin/server.js", "--param1", "arg1"]
  command: ["airflow"]
  args: ["webserver", "--pid" , "/tmp/airflow-webserver.pid"]
  requests:
    cpu: 1000m
    memory: 2Gi
  limits:
    cpu: 1000m
    memory: 2Gi
  liveness_probe:
    tcpSocket:
      port: *airflow_frontend
  readiness_probe:
    httpGet:
      path: /health
      port: *airflow_frontend

  volumes:
  - name: airflow-config
    configMap:
      name: airflow-config
  - name: airflow-webserver-config
    configMap:
      name: airflow-webserver-config
  - name: airflow-bash-executables
    configMap:
      name: airflow-bash-executables
      defaultMode: 0777
  - name: airflow-kerberos-client-config
    configMap:
      name: airflow-kerberos-client-config
  - name: airflow-connections-variables
    secret:
      secretName: airflow-connections-variables
  - name: airflow-kubernetes-executor-pod-template
    configMap:
      name: airflow-kubernetes-executor-pod-template
  - name: logs
    emptyDir: {}
  - name: airflow-dags
    persistentVolumeClaim:
      claimName: airflow-dags-pvc
  - name: airflow-kerberos-keytab
    secret:
      secretName: airflow-kerberos-keytab
  - name: airflow-kerberos-token
    persistentVolumeClaim:
      claimName: airflow-kerberos-token-pvc

  volumeMounts: &airflow_volume_mounts
  - name: airflow-config
    mountPath: /opt/airflow/airflow.cfg
    subPath: airflow.cfg
  - name: airflow-webserver-config
    mountPath: /opt/airflow/webserver_config.py
    subPath: webserver_config.py
  - name: logs
    mountPath: /opt/airflow/logs
  - name: airflow-kerberos-client-config
    mountPath: /etc/krb5.conf
    subPath: krb5.conf
  - name: airflow-bash-executables
    mountPath: /opt/airflow/usr/bin
  - name: airflow-connections-variables
    mountPath: /opt/airflow/secrets
  - name: airflow-dags
    readOnly: true
    mountPath: &dags_root /opt/airflow/dags
  - name: airflow-kubernetes-executor-pod-template
    mountPath: &pod_template_file /opt/airflow/pod_templates/pod_template.yaml
    subPath: pod_template.yaml
  - name: airflow-kerberos-keytab
    mountPath: /etc/kerberos/keytabs
    readOnly: true
  - name: airflow-kerberos-token
    mountPath: /tmp/airflow_krb5_ccache


service:
  deployment: minikube # valid values are "production" and "minikube"
  port:
    name: http # a unique name of lowercase alphanumeric characters or "-", starting and ending with alphanumeric, max length 63
    # protocol: TCP # TCP is the default protocol
    targetPort: *airflow_frontend # the number or name of the exposed port on the container
    port: *airflow_frontend # the number of the port desired to be exposed to the cluster
    nodePort: null # you need to define this if "production" is used. In minikube environments let it autoallocate

config:
  public: # Add here all the keys that can be publicly available as a ConfigMap
    AIRFLOW_HOME: /opt/airflow
  private: {} # Add here all the keys that should be private but still available as env variables
  airflow:
    postgresqlPass: secret
    dbHost: ''
    dbName: ''
    dbUser: ''
    dags_root: *dags_root
    dags_folder: override_me
    instance_name: override_me
    aws_access_key_id: override_me
    aws_secret_access_key: override_me
    s3_bucket: &s3_bucket '{{ printf "logs.airflow-%s.%s" (replace "_" "-" $.Values.config.airflow.instance_name) $.Values.environmentName }}'

    auth:
      role_admin: Admin
      roles_sync_at_login: true
      user_registration: true
      user_registration_role: Public
      role_mappings:
        nda: [User]
        wmf: [User]
        ops: [Admin]

    # https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html
    config:
      api: {}
      core:
        colored_console_log: false
        dags_folder: "{{ $.Values.config.airflow.dags_root }}/{{ $.Values.gitsync.link_dir }}/{{ $.Values.config.airflow.dags_folder }}"
        plugins_folder: "{{ $.Values.config.airflow.dags_root }}/{{ $.Values.gitsync.link_dir }}/wmf_airflow_common/plugins"
        executor: LocalExecutor
        load_examples: false
        max_active_runs_per_dag: 3
        max_active_tasks_per_dag: 6
        default_task_retries: 5
        parallelism: 64
        test_connection: enabled
        auth_manager: webserver_config.CustomAuthManager
      database:
        load_default_connections: false
      kerberos:
        reinit_frequency: 3600 # in seconds
        ccache: /tmp/airflow_krb5_ccache/krb5cc
        include_ip: false
        keytab: /etc/kerberos/keytabs/airflow.keytab
        principal: override_me/override_me
      kubernetes_executor:
        multi_namespace_mode: false
        namespace: "{{ $.Release.Namespace }}"
        pod_template_file: *pod_template_file
        worker_container_repository: "{{ $.Values.docker.registry }}/{{ $.Values.app.executor_pod_image }}"
        worker_container_tag: "{{ $.Values.app.executor_pod_image_version }}"
      logging:
        colored_console_log: false
        remote_logging: true
        delete_local_logs: true
        remote_base_log_folder: 's3://{{ $.Values.config.airflow.s3_bucket }}'
        remote_log_conn_id: s3_dpe
        encrypt_s3_logs: false
        logging_level: INFO
        fab_logging_level: INFO
      metrics:
        metrics_allow_list: operator_failures_,operator_successes_,sla_missed,executor.queued_tasks,dag.,dagrun.duration.,scheduler.scheduler_loop_duration,dag_processing.import_errors,dag_processing.total_parse_time,ti.failures,ti.successes,ti.finish,ti_failures,ti_successes
        statsd_custom_client_path: wmf_airflow_common.metrics.custom_statsd_client.CustomStatsClient
        statsd_host: localhost
        statsd_on: True
        statsd_port: "{{ $.Values.monitoring.statsd.port }}"
        statsd_prefix: airflow
      scheduler:
        standalone_dag_processor: false
        enable_health_check: true
        scheduler_health_check_server_host: '0.0.0.0'
        scheduler_health_check_server_port: &scheduler_healthcheck_port 8765
      secrets:
        backend: airflow.secrets.local_filesystem.LocalFilesystemBackend
        # See https://github.com/apache/airflow/blob/73dd6c17bd10ddda63a1682ac2174b0d206590dd/airflow/secrets/local_filesystem.py#L278-L279
        backend_kwargs: '{"connections_file_path": "/opt/airflow/secrets/connections.yaml"}' # WARN: must be json-parseable
      email:
        default_email_on_retry: false # T377745. We want to be alerted only after exhausting all retries.
      smtp:
        smtp_host: mx-out1001.wikimedia.org
        smtp_mail_from: "airflow-{{ $.Values.config.airflow.instance_name }}: <noreply@wikimedia.org>"
        smtp_port: 25
        smtp_ssl: false
        smtp_starttls: false
      triggerer:
        default_capacity: 1000
      webserver:
        enable_proxy_fix: true
        rbac: true
        base_url: "https://{{ $.Values.ingress.gatewayHosts.default }}.wikimedia.org"
        # Our deployments are public-facing by default, but secured by OIDC authentication. Therefore, we can disable this warning. See #T375739
        warn_deployment_exposure: false
        expose_config: non-sensitive-only
        instance_name: "{{ $.Values.config.airflow.instance_name }}"
        expose_hostname: true
        expose_stacktrace: true

  # https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html
  connections:
    fs_local:
      conn_type: fs
      description: Local filesystem on the Airflow Scheduler node
    s3_dpe:
      conn_type: aws
      login: "{{ $.Values.config.airflow.aws_access_key_id }}"
      password: "{{ $.Values.config.airflow.aws_secret_access_key }}"
      extra:
        verify: /etc/ssl/certs/wmf-ca-certificates.crt
    spark_yarn_cluster:
      conn_type: spark
      host: yarn
      extra:
        deploy-mode: cluster

  oidc:
    idp_server: override_me
    client_id: override_me
    client_secret: override_me

scheduler:
  enabled: true
  service_name: airflow-scheduler
  local_executor_api_port: 8793
  liveness_probe:
    tcpSocket:
      port: *scheduler_healthcheck_port
  readiness_probe:
    httpGet:
      path: /health
      port: *scheduler_healthcheck_port
  requests:
    cpu: 1000m
    memory: 2Gi
  limits:
    cpu: 1000m
    memory: 2Gi
  volumeMounts: *airflow_volume_mounts

# Export the content of the postgresql app secret to environment variables
postgresql:
  cloudnative: true
  secrets:
    uri: PG_URI
    host: PG_HOST

gitsync:
  enabled: true
  image_name: repos/data-engineering/git-sync
  image_tag: override_me
  image_gid: 900
  dags_repo: "https://gitlab.wikimedia.org/repos/data-engineering/airflow-dags.git"
  root_dir: /dags
  link_dir: airflow_dags
  ref: main
  period: 300 # git pull every 5 minutes
  volume:
    storage_class: override_me
    size: 100Mi

worker:
  resources:
    requests:
      cpu: 1000m
      memory: 2Gi
    limits:
      cpu: 2000m
      memory: 4Gi
  config:
    hadoop:
      hdfs:
        dfs.permissions.superusergroup: hadoop
        dfs.datanode.failed.volumes.tolerated: '2'
        dfs.ha.automatic-failover.enabled: 'true'
        dfs.namenode.service.handler.count: '100'
        dfs.nameservices: override_me
        dfs.internal.nameservices: override_me
        dfs.namenode.shared.edits.dir: override_me
        dfs.journalnode.edits.dir: /var/lib/hadoop/journal
        dfs.ha.fencing.methods: shell(/bin/true)
        dfs.namenode.name.dir: file:///var/lib/hadoop/name
        dfs.namenode.handler.count: '127'
        dfs.namenode.quota.init-threads: '16'
        dfs.block.size: '268435456'
        dfs.blocksize: '268435456'
        dfs.datanode.hdfs-blocks-metadata.enabled: 'true'
        dfs.datanode.fsdataset.volume.choosing.policy: org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy
        dfs.webhdfs.enabled: 'false'
        dfs.namenode.audit.log.async: 'true'
        dfs.block.access.token.enable: 'true'
        dfs.data.transfer.protection: privacy
        dfs.datanode.kerberos.principal: hdfs/_HOST@WIKIMEDIA
        dfs.datanode.keytab.file: /etc/security/keytabs/hadoop/hdfs.keytab
        dfs.encrypt.data.transfer: 'true'
        dfs.encrypt.data.transfer.cipher.key.bitlength: '128'
        dfs.encrypt.data.transfer.cipher.suites: AES/CTR/NoPadding
        dfs.http.policy: HTTPS_ONLY
        dfs.journalnode.kerberos.internal.spnego.principal: HTTP/_HOST@WIKIMEDIA
        dfs.journalnode.kerberos.principal: hdfs/_HOST@WIKIMEDIA
        dfs.journalnode.keytab.file: /etc/security/keytabs/hadoop/hdfs.keytab
        dfs.namenode.kerberos.principal: hdfs/_HOST@WIKIMEDIA
        dfs.namenode.keytab.file: /etc/security/keytabs/hadoop/hdfs.keytab
        dfs.secondary.namenode.kerberos.principal: hdfs/_HOST@WIKIMEDIA
        dfs.secondary.namenode.keytab.file: /etc/security/keytabs/hadoop/hdfs.keytab
        dfs.web.authentication.kerberos.keytab: /etc/security/keytabs/hadoop/HTTP.keytab
        dfs.web.authentication.kerberos.principal: HTTP/_HOST@WIKIMEDIA
      core:
        fs.defaultFS: override_me
        ha.zookeeper.quorum: override_me
        io.file.buffer.size: '131072'
        hadoop.proxyuser.mapred.hosts: '*'
        hadoop.proxyuser.mapred.groups: '*'
        fs.trash.checkpoint.interval: '1440'
        fs.trash.interval: '43200'
        net.topology.script.file.name: /usr/local/bin/generate_net_topology.sh
        fs.permissions.umask-mode: '027'
        hadoop.http.staticuser.user: yarn
        hadoop.rpc.protection: privacy
        hadoop.security.authentication: kerberos
        hadoop.ssl.enabled.protocols: TLSv1.2
      yarn:
        spark.authenticate: 'true'
        spark.network.crypto.enabled: 'true'
        yarn.acl.enable: 'true'
        yarn.admin.acl: yarn analytics-admins
        yarn.app.mapreduce.am.env: LD_LIBRARY_PATH=/usr/lib/hadoop/lib/native
        yarn.app.mapreduce.am.scheduler.connection.wait.interval-ms: '5000'
        yarn.app.mapreduce.am.staging-dir: /user
        yarn.application.classpath: $HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,$HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,$HADOOP_MAPRED_HOME/*,$HADOOP_MAPRED_HOME/lib/*,$HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*
        yarn.log-aggregation-enable: 'true'
        yarn.log-aggregation.retain-check-interval-seconds: '86400'
        yarn.log-aggregation.retain-seconds: '5184000'
        yarn.node-labels.enabled: 'true'
        yarn.node-labels.fs-store.root-dir: override_me
        yarn.nodemanager.log-dirs: ''
        yarn.nodemanager.remote-app-log-dir: /var/log/hadoop-yarn/apps
        yarn.nodemanager.log-aggregation.compression-type: gz
        yarn.resourcemanager.am.max-attempts: '6'
        yarn.resourcemanager.cluster-id: override_me
        yarn.resourcemanager.connect.retry-interval.ms: '2000'
        yarn.resourcemanager.ha.automatic-failover.embedded: 'true'
        yarn.resourcemanager.ha.automatic-failover.enabled: 'true'
        yarn.resourcemanager.ha.enabled: 'true'
        yarn.resourcemanager.ha.rm-ids: override_me
        yarn.resourcemanager.max-completed-applications: '5000'
        yarn.resourcemanager.principal: yarn/_HOST@WIKIMEDIA
        yarn.resourcemanager.recovery.enabled: 'true'
        yarn.resourcemanager.scheduler.class: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
        yarn.resourcemanager.scheduler.monitor.enable: 'true'
        yarn.resourcemanager.store.class: org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore
        yarn.resourcemanager.work-preserving-recovery.enabled: 'true'
        yarn.resourcemanager.zk-address: override_me
        yarn.resourcemanager.zk-state-store.parent-path: /yarn-rmstore/analytics-hadoop
        yarn.resourcemanager.zk-timeout-ms: '20000'
        yarn.scheduler.maximum-allocation-mb: '49152'
        yarn.scheduler.maximum-allocation-vcores: '32'
        yarn.scheduler.minimum-allocation-mb: '1'
        yarn.scheduler.minimum-allocation-vcores: '1'
    spark:
      spark:
        spark.yarn.historyServer.address: override_me
        spark.dynamicAllocation.enabled: true
        spark.shuffle.service.enabled: true
        spark.dynamicAllocation.executorIdleTimeout: 60s
        spark.dynamicAllocation.cachedExecutorIdleTimeout: 3600s
        spark.shuffle.io.maxRetries: 10
        spark.shuffle.io.retryWait: 10s
        spark.executorEnv.LD_LIBRARY_PATH: /usr/lib/hadoop/lib/native
        spark.sql.catalogImplementation: hive
        spark.sql.extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
        spark.sql.catalog.spark_catalog: org.apache.iceberg.spark.SparkSessionCatalog
        spark.sql.catalog.spark_catalog.type: hive
        spark.driver.port: 12000
        spark.port.maxRetries: 100
        spark.ui.port: 4040
        spark.driver.blockManager.port: 13000
        spark.sql.files.maxPartitionBytes: '268435456'
        spark.sql.warehouse.dir: hdfs:///user/hive/warehouse
        spark.yarn.archive: hdfs:///user/spark/share/lib/spark-3.1.2-assembly.jar
        spark.driver.defaultJavaOptions: -Djava.net.useSystemProxies=True
        spark.executor.defaultJavaOptions: -Djava.net.useSystemProxies=True
        spark.authenticate: true
        spark.network.crypto.enabled: true
        spark.network.crypto.keyFactoryAlgorithm: PBKDF2WithHmacSHA256
        spark.network.crypto.keyLength: 256
        spark.network.crypto.saslFallback: false
        spark.eventLog.enabled: true
        spark.eventLog.dir: hdfs:///var/log/spark
        spark.eventLog.compress: true
      hive:
        hive.metastore.uris: override_me
        hive.metastore.kerberos.keytab.file: override_me
        hive.metastore.kerberos.principal: override_me
        hive.server2.authentication.kerberos.principal: override_me
        hive.server2.authentication.kerberos.keytab: override_me
        hive.cluster.delegation.token.store.class: org.apache.hadoop.hive.thrift.DBTokenStore
        hive.metastore.sasl.enabled: 'true'
        hive.server2.thrift.sasl.qop: auth-conf
        hive.support.concurrency: 'false'
        hive.metastore.disallow.incompatible.col.type.changes: 'false'
        hive.metastore.execute.setugi: 'true'
        hive.cli.print.current.db: 'true'
        hive.cli.print.header: 'true'
        hive.mapred.mode: strict
        hive.start.cleanup.scratchdir: 'true'
        hive.exec.stagingdir: /tmp/hive-staging
        hive.error.on.empty.partition: 'true'
        hive.exec.parallel: 'true'
        hive.exec.parallel.thread.number: '8'
        hive.stats.autogather: 'false'
        hive.variable.substitute.depth: '10000'
        hive.aux.jars.path: file:///usr/lib/hive-hcatalog/share/hcatalog/hive-hcatalog-core.jar
        hive.default.fileformat: parquet
        parquet.compression: SNAPPY
        hive.server2.builtin.udf.blacklist: xpath,xpath_string,xpath_boolean,xpath_number,xpath_double,xpath_float,xpath_long,xpath_int,xpath_short
        hive.resultset.use.unique.column.names: 'false'
        hive.exec.submit.local.task.via.child: 'false'
        hive.server2.authentication: KERBEROS
        hive.server2.logging.operation.enabled: 'true'
  volumes:
  - name: airflow-hadoop-configuration
    configMap:
      name: airflow-hadoop-configuration
  - name: airflow-spark-configuration
    configMap:
      name: airflow-spark-configuration
  volumeMounts:
  - name: airflow-hadoop-configuration
    mountPath: /etc/hadoop/conf
  - name: airflow-spark-configuration
    mountPath: /etc/spark3/conf


kerberos:
  enabled: true
  keytab: ~
  volume:
    size: 10Mi
    storage_class: override_me
  image_gid: 900
  resources:
    requests:
      cpu: 100m
      memory: 200Mi
    limits:
      cpu: 500m
      memory: 400Mi


# Additional resources if we want to add a port for a debugger to connect to.
debug:
  enabled: false
  # Define here any port that you want to expose for debugging purposes
  ports: []

# Allow external traffic to reach this service via a (cluster provided) ingress controller.
# https://wikitech.wikimedia.org/wiki/Kubernetes/Ingress#Configuration_(for_service_owners)
ingress:
  enabled: true
  # By default, enabling ingress will switch the charts services from type NodePort to
  # ClusterIP. While that is fine for new services it may not be desired during transition
  # of existing ones from dedicated LVS to Ingress.
  # By setting keepNodePort to true, the services will stay of type NodePort.
  keepNodePort: false
  # gatewayHosts settings configure the hostnames this service will be reachable on.
  # By default, this will be a list like:
  # - {{ gatewayHosts.default }}.{{ domain }}
  # For all domains listed in .gatewayHosts.domains (specified by SRE for each environment)
  gatewayHosts:
    # default will expand to {{ .Release.Namespace }} as long as it is an empty string.
    default: ""
    # disableDefaultHosts may be set to true if the service should not be reachable via
    # the gateway hosts generated by default (see above).
    disableDefaultHosts: false
    # extraFQDNs ist a list of extra FQDNs this service should be reachable on.
    # It can be used to extend the gateway hosts that are generated by default.
    extraFQDNs: []
  # If you want to attach routes of this release to an existing Gateway, provide the name
  # of that gateway here in the format: <namespace>/<gateway-name>
  # This is useful if you wish to make multiple releases available from the same hostname.
  existingGatewayName: ""
  # routeHosts is a list of FQDNs the httproutes should attach to.
  # If existingGatewayName not set, this list might be empty and will default to the gateway
  # host generated according to how .Values.gatewayHosts.* is configured.
  # If existingGatewayName is set, you need to provide the FQDNs your routes should attach to.
  routeHosts: []
  # HTTPRoute routing rules. By default https://<hosts from above>/ will be routed to
  # the service without modification.
  # Docs: https://istio.io/v1.9/docs/reference/config/networking/virtual-service/#HTTPRoute
  httproutes: []
  # Base CORS HTTP headers for the general use case.
  base_cors_policy: false
  # Add a custom CORS policy, injecting an Istio CorsPolicy config:
  # https://istio.io/latest/docs/reference/config/networking/virtual-service/#CorsPolicy
  # Takes precedence over base_cors_policy.
  custom_cors_policy: {}

# Basic mesh-related data.
mesh:
  enabled: true
  certmanager:
    enabled: true
  admin: {port: 1666 }
  image_version: latest
  # http keepalive timeout for incoming requests
  idle_timeout: "4.5s"
  # Port to listen to
  public_port: 1025 # arbitrary placeholder
  local_access_log_min_code: "200"
  # Headers to add to a local request,
  # in dictionary form.
  request_headers_to_add: []
  # Timeout of a request to the local service
  upstream_timeout: "60s"
  # Enabling telemetry, telemetry port.
  telemetry:
    enabled: true
    port: 1667
  extra_service_selector_labels:
    component: webserver
  resources:
    requests:
      cpu: 200m
      memory: 100Mi
    limits:
      cpu: 500m
      memory: 500Mi

# Mesh-related discovery
# TODO: move under mesh.* once we can
discovery:
  # List of listeners
  listeners: []

# Mesh related pure TCP proxies
tcp_proxy:
  listeners: []

# Should be provided by configuration management.
# See details of the structures in the comments
# In the configuration module.
services_proxy: ~
tcp_services_proxy: ~

common_images:
  statsd:
    exporter: prometheus-statsd-exporter:latest
# WARNING: If you want to enable the module,
# you will need to add a "statsd" stanza to monitoring
# see modules/base/values.yaml for reference.
  kerberos:
    image: repos/data-engineering/kerberos-kinit
    version: latest

docker:
  registry: docker-registry.wikimedia.org
  pull_policy: IfNotPresent
resources:
  replicas: 1

monitoring:
  # If enabled is true, monitoring annotations will be added to the deployment.
  enabled: true
  prometheus_port: 9102
  statsd:
    port: 9125
    prestop_sleep: 0
    requests:
      memory: 100Mi
      cpu: 100m
    limits:
      memory: 200Mi
      cpu: 200m
    # from https://github.com/blueswen/gunicorn-monitoring
    config: |-
      mappings:
      - match: airflow.scheduler.scheduler_loop_duration
        name: airflow_scheduler_loop_duration
        summary_options:
          quantiles:
            - quantile: 0.5
              error: 0.1
          max_age: 30s
          age_buckets: 30
          buf_cap: 10

      # Example:
      #   airflow.operator_failures__HdfsEmailOperator count
      #   => airflow_operator{operator="HdfsEmailOperator", state="failures"} count
      - match: airflow\.operator_(failures|successes)[\._]+(\w+)
        match_type: regex
        name: airflow_operator
        ttl: 1m
        labels:
          state: "$1"
          operator: "$2"

      # Example:
      #   airflow.ti_failures count
      #   => airflow_ti{state="failures"} count
      - match: airflow\.ti_(failures|successes)
        match_type: regex
        name: airflow_ti
        labels:
          state: "$1"

      # Example:
      #   airflow.ti.start.example_python_operator.print_array count
      #   dropped
      - match: airflow\.ti\.start.*
        match_type: regex
        name: airflow_ti_start
        action: drop


      # Renamed to remove ambiguity with the next mapping
      # The ambiguity only happens in prod, and depends on the order of the metric arrivals to the exporter.
      - match: airflow\.ti\.finish$
        match_type: regex
        name: airflow_ti_finish_aggregated

      # Example:
      #   airflow.ti.finish.example_dag.task_1.failed count
      #   => airflow_ti_finish{dag_id="example_dag", task_id="task1", state="failed"} count
      - match: airflow\.ti\.finish.(\w+)\.(\w+)\.(queued|running|scheduled|success|failed)
        match_type: regex
        name: airflow_ti_finish
        ttl: 1m
        labels:
          dag_id: "$1"
          task_id: "$2"
          state: "$3"

      # Example:
      #   airflow.ti.finished.example_python_operator.print_array.None count
      #   dropped
      - match: airflow\.ti\.finish\.(\w+)\.(\w+)\.(None|deferred|removed|restarting|shutdown|skipped|up_for_reschedule|up_for_retry|upstream_failed)
        match_type: regex
        name: airflow_ti_finish_useless
        action: drop

      # Example:
      #   airflow.dag.pageview_hourly.move_data_to_archive.duration
      #   => airflow_dag_duration{dag_id="pageview_hourly", task_id="move_data_to_archive"} count
      - match: airflow\.dag\.(\w+)\.(\w+)\.duration
        match_type: regex
        name: "airflow_task_duration"
        ttl: 1m
        labels:
          dag_id: "$1"
          task_id: "$2"

      # Example:
      #   airflow.dag.pageview_hourly.move_data_to_archive.duration
      #   dropped
      - match: airflow\.dag\.(\w+)\.(\w+)\.(queued_duration|scheduled_duration)
        match_type: regex
        name: airflow_dag_other_durations
        action: drop

      # Example:
      #   airflow.dagrun.duration.pageview_hourly count
      #   dropped
      - match: airflow\.dagrun\.duration\.(success|failed)$
        match_type: regex
        name: airflow_dagrun_duration_success
        action: drop

      # Example:
      #   airflow.dagrun.duration.success.pageview_hourly count
      #   => airflow_dagrun_duration{dag_id="pageview_hourly", state="success"} count
      - match: airflow\.dagrun\.duration\.(success|failed)\.(\w+)
        match_type: regex
        name: airflow_dagrun_duration
        ttl: 1m
        labels:
          state: "$1"
          dag_id: "$2"

networkpolicy:
  egress:
    enabled: true

# Add here the list of kafka-clusters (by name) that the service will need to reach.
kafka:
  allowed_clusters: []

# Optional affinity settings
affinity: {}
#  affinity:
#    nodeAffinity:
#      requiredDuringSchedulingIgnoredDuringExecution:
#        nodeSelectorTerms:
#          - matchExpressions:
#              - key: some-key
#                operator: In
#                values:
#                  - some-value
#  nodeSelector:
#    node.kubernetes.io/some-key: some-value

# Cronjob definitions
# Here you can define your cronjobs
cronjobs: []
# Example of a job:
# - name: my-cron-hourly
#   enabled: true
#   command:
#      - /bin/cowsay
#      - "hello"
#   schedule: "@hourly" (defaults to @daily)
#   concurrency: Replace (defaults to "Forbid")
#   image_versioned: my-app:1.1.1 (defaults to the app used in the main application definition)
#   resources: (optional list of requests/limits for our cronjob; if not present will default to the application ones.)

# The set of external services to allow egress to
# Example:
# kafka:
# - main-codfw
# - main-eqiad
# presto:
# - analytics
#
# See https://wikitech.wikimedia.org/wiki/Kubernetes/Deployment_Charts#Enabling_egress_to_services_external_to_Kubernetes
# for the list of supported services
external_services: {}

pgServiceName: override_me
environmentName: override_me

# Definition of extra IP: hostnames to be injected in /etc/hosts
# Typically, we use this to force the reverse DNS resolution of an an-master host
# IP to its an-masterxxxx.eqiad.wmnet name and to avoid the coredns service ip reverse
# DNS, to validate Kerberos identity.
host_aliases: {}
