# Default values.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# This chart defines the k8s resources needed for Airflow, but NOT for its database.
# A database needs to be deployed alongside with its own mechanism

_vars:
  krb5ccdir: /tmp/airflow_krb5_ccache
  krb5ccname: &krb5ccname /tmp/airflow_krb5_ccache/krb5cc
  krb5conf: &krb5conf /etc/krb5.conf
  keytab: &keytab /etc/kerberos/keytabs/airflow.keytab
  hadoop_conf_dir: &hadoop_conf_dir /etc/hadoop/conf
  spark_conf_dir: &spark_conf_dir /etc/spark3/conf
  hdfs_keytab: &hdfs_keytab /etc/security/keytabs/hadoop/hdfs.keytab
  hdfs_principal: &hdfs_principal hdfs/_HOST@WIKIMEDIA

app:
  image: &base-image repos/data-engineering/airflow-dags
  version: &base-image-version latest # we use latest everywhere in the defaults.
  executor_pod_image: *base-image
  executor_pod_image_version: *base-image-version
  port: &airflow_frontend 8080 # airflow webUI
  # port: *airflow_frontend # port exposed as a Service, also used by service-checker.
  # Use command and args below to override the entrypoint. Type is arrays
  # Not necessary unless you want to change the entrypoint defined in the docker image
  # Example:
  # command: ["node"]
  # args: ["bin/server.js", "--param1", "arg1"]
  command: ["airflow"]
  args: ["webserver", "--pid" , "/tmp/airflow-webserver.pid"]
  requests:
    cpu: 1000m
    memory: 2Gi
  limits:
    cpu: 1000m
    memory: 2Gi
  liveness_probe:
    tcpSocket:
      port: *airflow_frontend
  readiness_probe:
    httpGet:
      path: /health
      port: *airflow_frontend

service:
  deployment: minikube # valid values are "production" and "minikube"
  port:
    name: http # a unique name of lowercase alphanumeric characters or "-", starting and ending with alphanumeric, max length 63
    # protocol: TCP # TCP is the default protocol
    targetPort: *airflow_frontend # the number or name of the exposed port on the container
    port: *airflow_frontend # the number of the port desired to be exposed to the cluster
    nodePort: null # you need to define this if "production" is used. In minikube environments let it autoallocate

config:
  public: # Add here all the keys that can be publicly available as a ConfigMap
    AIRFLOW_HOME: /opt/airflow
  private: {} # Add here all the keys that should be private but still available as env variables
  airflow:
    dags_root: /opt/airflow/dags
    dags_folder: '{{ $.Values.config.airflow.instance_name | replace "-" "_" }}'
    instance_name: override_me
    aws_access_key_id: override_me
    aws_secret_access_key: override_me
    s3_bucket: &s3_bucket '{{ printf "logs.airflow-%s.%s" (replace "_" "-" $.Values.config.airflow.instance_name) $.Values.environmentName }}'

    auth:
      role_admin: Admin
      roles_sync_at_login: true
      user_registration: true
      user_registration_role: Public
      op_ldap_role: "airflow-{{ $.Values.config.airflow.instance_name }}-ops"
      role_mappings:
        "{{ $.Values.config.airflow.auth.op_ldap_role }}": [Op]
        nda: [User]
        wmf: [User]
        ops: [Admin]

    extra_rbac: ~
    local_settings:
      ui_alerts: []
      xcom_sidecar:
        image: bookworm
        tag: latest
        resources:
          requests:
            cpu: 25m
            memory: 50Mi
          limits:
            cpu: 50m
            memory: 100Mi

    # https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html
    config:
      api: {}
      core:
        colored_console_log: false
        dags_folder: "{{ $.Values.config.airflow.dags_root }}/{{ $.Values.gitsync.link_dir }}/{{ $.Values.config.airflow.dags_folder }}"
        plugins_folder: "{{ $.Values.config.airflow.dags_root }}/{{ $.Values.gitsync.link_dir }}/wmf_airflow_common/plugins"
        executor: KubernetesExecutor
        load_examples: false
        max_active_runs_per_dag: 3
        max_active_tasks_per_dag: 16
        default_task_retries: 5
        parallelism: 64
        test_connection: enabled
        auth_manager: webserver_config.CustomAuthManager
        xcom_backend: airflow.providers.common.io.xcom.backend.XComObjectStorageBackend
      common.io:
        # Store XCOMs larger than 500KB in S3 instead of database - T415661
        xcom_objectstorage_path: s3://{{ $.Values.config.airflow.config.logging.remote_log_conn_id }}@{{ $.Values.config.airflow.s3_bucket }}/xcoms
        xcom_objectstorage_threshold: "524288" # 500KB
        xcom_objectstorage_compression: ~ # We are planning to enable compression at the Ceph level down the road
      database:
        load_default_connections: false
      kerberos:
        reinit_frequency: 3600 # in seconds
        ccache: *krb5ccname
        include_ip: false
        keytab: *keytab
        principal: "analytics-{{ $.Values.config.airflow.instance_name | replace \"analytics-\" \"\" }}/airflow-{{ $.Values.config.airflow.instance_name }}.discovery.wmnet"
      kubernetes_executor:
        multi_namespace_mode: false
        namespace: "{{ $.Release.Namespace }}"
        pod_template_file: /opt/airflow/pod_templates/kubernetes_executor_default_pod_template.yaml
        worker_container_repository: "{{ $.Values.docker.registry }}/{{ $.Values.app.executor_pod_image }}"
        worker_container_tag: "{{ $.Values.app.executor_pod_image_version }}"
        # Number of pods that the k8s executor can create in a single scheduler loop
        # See https://airflow.apache.org/docs/apache-airflow-providers-cncf-kubernetes/stable/configurations-ref.html#worker-pods-creation-batch-size
        worker_pods_creation_batch_size: 16
      logging:
        colored_console_log: false
        remote_logging: true
        delete_local_logs: true
        remote_base_log_folder: 's3://{{ $.Values.config.airflow.s3_bucket }}'
        remote_log_conn_id: override_me
        encrypt_s3_logs: false
        logging_level: INFO
        fab_logging_level: INFO
      metrics:
        statsd_custom_client_path: wmf_airflow_common.metrics.custom_statsd_client.CustomStatsClient
        statsd_host: "{{ include \"service.statsd\" . }}"
        statsd_on: "{{ include \"toPythonValue\" (dict \"value\" $.Values.monitoring.enabled) }}"
        statsd_port: "{{ $.Values.monitoring.statsd.port }}"
        statsd_prefix: airflow
      scheduler:
        # Recommended in https://www.astronomer.io/docs/learn/airflow-scaling-workers/#core-settings -> we set the number of parsing processes to twice the amount of vcpus
        parsing_processes: "{{ max (mul (atoi (toString $.Values.scheduler.limits.cpu)) 2) 1 }}"
        scheduler_health_check_threshold: 60
        max_tis_per_query: 16
        # pool metrics can be expensive to compute. Instead of re-generating them every 5s (default), we align their
        # computation with the default prometheus scraping interval, to avoid needless rollups in statsd.
        pool_metrics_interval: 60
        standalone_dag_processor: false
        enable_health_check: true
        scheduler_health_check_server_host: '0.0.0.0'
        scheduler_health_check_server_port: &scheduler_healthcheck_port 8765
      secrets:
        backend: airflow.secrets.local_filesystem.LocalFilesystemBackend
        # See https://github.com/apache/airflow/blob/73dd6c17bd10ddda63a1682ac2174b0d206590dd/airflow/secrets/local_filesystem.py#L278-L279
        backend_kwargs: '{"connections_file_path": "/opt/airflow/secrets/connections.yaml"}' # WARN: must be json-parseable
      email:
        default_email_on_retry: false # T377745. We want to be alerted only after exhausting all retries.
      smtp:
        smtp_host: override_me
        smtp_mail_from: "airflow-{{ $.Values.config.airflow.instance_name }}: <noreply@wikimedia.org>"
        smtp_port: 25
        smtp_ssl: false
        smtp_starttls: false
      triggerer:
        default_capacity: 1000
      webserver:
        enable_proxy_fix: true
        rbac: true
        base_url: "https://{{ $.Values.ingress.gatewayHosts.default }}.wikimedia.org"
        # Our deployments are public-facing by default, but secured by OIDC authentication. Therefore, we can disable this warning. See #T375739
        warn_deployment_exposure: false
        expose_config: non-sensitive-only
        instance_name: "{{ $.Values.config.airflow.instance_name }}"
        expose_hostname: true
        expose_stacktrace: true
        # We wish to allow long-running requests, such as when clearing many tasks. See #T400493
        web_server_worker_timeout: 300 # Note: Deprecated in version 3.0 and replaced with: api.worker_timeout

  # https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html
  connections:
    fs_local:
      conn_type: fs
      description: Local filesystem on the Airflow Scheduler node
    s3_dpe:
      conn_type: aws
      login: "{{ $.Values.config.airflow.aws_access_key_id }}"
      password: "{{ $.Values.config.airflow.aws_secret_access_key }}"
      extra:
        verify: /etc/ssl/certs/wmf-ca-certificates.crt
    spark_yarn_cluster:
      conn_type: spark
      host: yarn
      extra:
        deploy-mode: cluster

  extra_secrets: {}

  oidc:
    idp_server: override_me
    client_id: 'airflow_{{ $.Values.config.airflow.instance_name | replace "-" "_" }}'
    client_secret: override_me

scheduler:
  enabled: true
  service_name: airflow-scheduler
  local_executor_api_port: 8793
  liveness_probe:
    tcpSocket:
      port: *scheduler_healthcheck_port
    initialDelaySeconds: 10
    periodSeconds: 30
    timeoutSeconds: 10
  readiness_probe:
    tcpSocket:
      port: *scheduler_healthcheck_port
  requests:
    cpu: "1"
    memory: 2Gi
  limits:
    cpu: "1"
    memory: 2Gi

triggerer:
  port: &triggerer_port 8794
  liveness_probe:
    tcpSocket:
      port: *triggerer_port
    initialDelaySeconds: 10
    periodSeconds: 30
    timeoutSeconds: 10
  readiness_probe:
    tcpSocket:
      port: *triggerer_port
  enabled: false
  requests:
    cpu: "0.5"
    memory: 1Gi
  limits:
    cpu: "0.5"
    memory: 2Gi

# Export the content of the postgresql app secret to environment variables
postgresql:
  secrets:
    uri: PG_URI
    host: PG_HOST

gitsync:
  enabled: true
  image_name: repos/data-engineering/git-sync
  image_tag: override_me
  image_gid: 900
  dags_repo: "https://gitlab.wikimedia.org/repos/data-engineering/airflow-dags.git"
  root_dir: /dags
  link_dir: airflow_dags
  ref: main
  period: 300 # git pull every 5 minutes
  volume:
    storage_class: override_me
    size: 100Mi

worker:
  proxy:
    urldownloader:
      enabled: false
  resources:
    requests:
      cpu: 1000m
      memory: 1500Mi
    limits:
      cpu: 2000m
      memory: 3Gi
  config:
    extra_files: {}
    hadoop:
      hdfs:
        dfs.permissions.superusergroup: hadoop
        dfs.datanode.failed.volumes.tolerated: '2'
        dfs.ha.automatic-failover.enabled: 'true'
        dfs.namenode.service.handler.count: '100'
        dfs.nameservices: override_me
        dfs.internal.nameservices: override_me
        dfs.namenode.shared.edits.dir: override_me
        dfs.journalnode.edits.dir: /var/lib/hadoop/journal
        dfs.ha.fencing.methods: shell(/bin/true)
        dfs.namenode.name.dir: file:///var/lib/hadoop/name
        dfs.namenode.handler.count: '127'
        dfs.namenode.quota.init-threads: '16'
        dfs.block.size: '268435456'
        dfs.blocksize: '268435456'
        dfs.datanode.hdfs-blocks-metadata.enabled: 'true'
        dfs.datanode.fsdataset.volume.choosing.policy: org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy
        dfs.webhdfs.enabled: 'false'
        dfs.namenode.audit.log.async: 'true'
        dfs.block.access.token.enable: 'true'
        dfs.data.transfer.protection: privacy
        dfs.datanode.kerberos.principal: *hdfs_principal
        dfs.datanode.keytab.file: *hdfs_keytab
        dfs.encrypt.data.transfer: 'true'
        dfs.encrypt.data.transfer.cipher.key.bitlength: '128'
        dfs.encrypt.data.transfer.cipher.suites: AES/CTR/NoPadding
        dfs.http.policy: HTTPS_ONLY
        dfs.journalnode.kerberos.internal.spnego.principal: HTTP/_HOST@WIKIMEDIA
        dfs.journalnode.kerberos.principal: *hdfs_principal
        dfs.journalnode.keytab.file: *hdfs_keytab
        dfs.namenode.kerberos.principal: *hdfs_principal
        dfs.namenode.keytab.file: *hdfs_keytab
        dfs.secondary.namenode.kerberos.principal: *hdfs_principal
        dfs.secondary.namenode.keytab.file: *hdfs_keytab
        dfs.web.authentication.kerberos.keytab: /etc/security/keytabs/hadoop/HTTP.keytab
        dfs.web.authentication.kerberos.principal: *hdfs_principal
      core:
        fs.defaultFS: override_me
        ha.zookeeper.quorum: override_me
        io.file.buffer.size: '131072'
        hadoop.proxyuser.mapred.hosts: '*'
        hadoop.proxyuser.mapred.groups: '*'
        fs.trash.checkpoint.interval: '1440'
        fs.trash.interval: '43200'
        net.topology.script.file.name: /usr/local/bin/generate_net_topology.sh
        fs.permissions.umask-mode: '027'
        hadoop.http.staticuser.user: yarn
        hadoop.rpc.protection: privacy
        hadoop.security.authentication: kerberos
        hadoop.ssl.enabled.protocols: TLSv1.2
      yarn:
        spark.authenticate: 'true'
        spark.network.crypto.enabled: 'true'
        yarn.acl.enable: 'true'
        yarn.admin.acl: yarn analytics-admins
        yarn.app.mapreduce.am.env: LD_LIBRARY_PATH=/usr/lib/hadoop/lib/native
        yarn.app.mapreduce.am.scheduler.connection.wait.interval-ms: '5000'
        yarn.app.mapreduce.am.staging-dir: /user
        yarn.application.classpath: $HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,$HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,$HADOOP_MAPRED_HOME/*,$HADOOP_MAPRED_HOME/lib/*,$HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*
        yarn.log-aggregation-enable: 'true'
        yarn.log-aggregation.retain-check-interval-seconds: '86400'
        yarn.log-aggregation.retain-seconds: '5184000'
        yarn.node-labels.enabled: 'true'
        yarn.node-labels.fs-store.root-dir: override_me
        yarn.nodemanager.log-dirs: ''
        yarn.nodemanager.remote-app-log-dir: /var/log/hadoop-yarn/apps
        yarn.nodemanager.log-aggregation.compression-type: gz
        yarn.resourcemanager.am.max-attempts: '6'
        yarn.resourcemanager.cluster-id: override_me
        yarn.resourcemanager.connect.retry-interval.ms: '2000'
        yarn.resourcemanager.ha.automatic-failover.embedded: 'true'
        yarn.resourcemanager.ha.automatic-failover.enabled: 'true'
        yarn.resourcemanager.ha.enabled: 'true'
        yarn.resourcemanager.ha.rm-ids: override_me
        yarn.resourcemanager.max-completed-applications: '5000'
        yarn.resourcemanager.principal: yarn/_HOST@WIKIMEDIA
        yarn.resourcemanager.recovery.enabled: 'true'
        yarn.resourcemanager.scheduler.class: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
        yarn.resourcemanager.scheduler.monitor.enable: 'true'
        yarn.resourcemanager.store.class: org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore
        yarn.resourcemanager.work-preserving-recovery.enabled: 'true'
        yarn.resourcemanager.zk-address: override_me
        yarn.resourcemanager.zk-state-store.parent-path: /yarn-rmstore/analytics-hadoop
        yarn.resourcemanager.zk-timeout-ms: '20000'
        yarn.scheduler.maximum-allocation-mb: '49152'
        yarn.scheduler.maximum-allocation-vcores: '32'
        yarn.scheduler.minimum-allocation-mb: '1'
        yarn.scheduler.minimum-allocation-vcores: '1'
      hive:
        hive.metastore.uris: override_me
        hive.metastore.kerberos.keytab.file: override_me
        hive.metastore.kerberos.principal: override_me
        hive.server2.authentication.kerberos.principal: override_me
        hive.server2.authentication.kerberos.keytab: override_me
        hive.cluster.delegation.token.store.class: org.apache.hadoop.hive.thrift.DBTokenStore
        hive.metastore.sasl.enabled: 'true'
        hive.server2.thrift.sasl.qop: auth-conf
        hive.support.concurrency: 'false'
        hive.metastore.disallow.incompatible.col.type.changes: 'false'
        hive.metastore.execute.setugi: 'true'
        hive.cli.print.current.db: 'true'
        hive.cli.print.header: 'true'
        hive.mapred.mode: strict
        hive.start.cleanup.scratchdir: 'true'
        hive.exec.stagingdir: /tmp/hive-staging
        hive.error.on.empty.partition: 'true'
        hive.exec.parallel: 'true'
        hive.exec.parallel.thread.number: '8'
        hive.stats.autogather: 'false'
        hive.variable.substitute.depth: '10000'
        hive.aux.jars.path: file:///usr/lib/hive-hcatalog/share/hcatalog/hive-hcatalog-core.jar
        hive.default.fileformat: parquet
        parquet.compression: SNAPPY
        hive.server2.builtin.udf.blacklist: xpath,xpath_string,xpath_boolean,xpath_number,xpath_double,xpath_float,xpath_long,xpath_int,xpath_short
        hive.resultset.use.unique.column.names: 'false'
        hive.exec.submit.local.task.via.child: 'false'
        hive.server2.authentication: KERBEROS
        hive.server2.logging.operation.enabled: 'true'
    spark:
      spark:
        spark.yarn.historyServer.address: override_me
        spark.dynamicAllocation.enabled: true
        spark.shuffle.service.enabled: true
        spark.dynamicAllocation.executorIdleTimeout: 60s
        spark.dynamicAllocation.cachedExecutorIdleTimeout: 3600s
        spark.shuffle.io.maxRetries: 10
        spark.shuffle.io.retryWait: 10s
        spark.executorEnv.LD_LIBRARY_PATH: /usr/lib/hadoop/lib/native
        spark.sql.catalogImplementation: hive
        spark.sql.extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
        spark.sql.catalog.spark_catalog: org.apache.iceberg.spark.SparkSessionCatalog
        spark.sql.catalog.spark_catalog.type: hive
        spark.driver.port: 12000
        spark.port.maxRetries: 100
        spark.ui.port: 4040
        spark.driver.blockManager.port: 13000
        spark.sql.files.maxPartitionBytes: '268435456'
        spark.sql.warehouse.dir: hdfs:///user/hive/warehouse
        spark.yarn.archive: hdfs:///user/spark/share/lib/spark-3.1.2-assembly.jar
        spark.driver.defaultJavaOptions: -Djava.net.useSystemProxies=True
        spark.executor.defaultJavaOptions: -Djava.net.useSystemProxies=True
        spark.authenticate: true
        spark.network.crypto.enabled: true
        spark.network.crypto.keyFactoryAlgorithm: PBKDF2WithHmacSHA256
        spark.network.crypto.keyLength: 256
        spark.network.crypto.saslFallback: false
        spark.eventLog.enabled: true
        spark.eventLog.dir: hdfs:///var/log/spark
        spark.eventLog.compress: true

  env:
    hadoop:
      - name: HADOOP_CONF_DIR
        value: *hadoop_conf_dir
      - name: HIVE_CONF_DIR
        value: *hadoop_conf_dir
    spark:
      - name: SPARK_CONF_DIR
        value: *spark_conf_dir

kubernetes_pod_operator:
  resources:
    requests:
      cpu: 1000m
      memory: 1500Mi
    limits:
      cpu: 1000m
      memory: 1500Mi

kerberos:
  enabled: true
  admin: override_me
  servers: [override_me]
  keytab: ~
  volume:
    size: 10Mi
    storage_class: override_me
  image_gid: 900
  resources:
    requests:
      cpu: 100m
      memory: 200Mi
    limits:
      cpu: 500m
      memory: 400Mi
  env:
    base:
    - name: KRB5CCNAME
      value: *krb5ccname
    - name: KRB5_CONFIG
      value: *krb5conf
    keytab:
    - name: KRB5_KEYTAB
      value: *keytab

hadoop_shell:
  enabled: true
  resources:
    requests:
      cpu: 200m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi

task_shell:
  enabled: true
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 256Mi

# Additional resources if we want to add a port for a debugger to connect to.
debug:
  enabled: false
  # Define here any port that you want to expose for debugging purposes
  ports: []

# Allow external traffic to reach this service via a (cluster provided) ingress controller.
# https://wikitech.wikimedia.org/wiki/Kubernetes/Ingress#Configuration_(for_service_owners)
ingress:
  enabled: true
  # By default, enabling ingress will switch the charts services from type NodePort to
  # ClusterIP. While that is fine for new services it may not be desired during transition
  # of existing ones from dedicated LVS to Ingress.
  # By setting keepNodePort to true, the services will stay of type NodePort.
  keepNodePort: false
  # gatewayHosts settings configure the hostnames this service will be reachable on.
  # By default, this will be a list like:
  # - {{ gatewayHosts.default }}.{{ domain }}
  # For all domains listed in .gatewayHosts.domains (specified by SRE for each environment)
  gatewayHosts:
    # default will expand to {{ .Release.Namespace }} as long as it is an empty string.
    default: ""
    # disableDefaultHosts may be set to true if the service should not be reachable via
    # the gateway hosts generated by default (see above).
    disableDefaultHosts: false
    # extraFQDNs is a list of extra FQDNs this service should be reachable on.
    # It can be used to extend the gateway hosts that are generated by default.
    extraFQDNs: []
  # If you want to attach routes of this release to an existing Gateway, provide the name
  # of that gateway here in the format: <namespace>/<gateway-name>
  # This is useful if you wish to make multiple releases available from the same hostname.
  existingGatewayName: ""
  # routeHosts is a list of FQDNs the httproutes should attach to.
  # If existingGatewayName not set, this list might be empty and will default to the gateway
  # host generated according to how .Values.gatewayHosts.* is configured.
  # If existingGatewayName is set, you need to provide the FQDNs your routes should attach to.
  routeHosts: []
  # HTTPRoute routing rules. By default https://<hosts from above>/ will be routed to
  # the service without modification.
  # Docs: https://istio.io/v1.9/docs/reference/config/networking/virtual-service/#HTTPRoute
  httproutes: []
  # Base CORS HTTP headers for the general use case.
  base_cors_policy: false
  # Add a custom CORS policy, injecting an Istio CorsPolicy config:
  # https://istio.io/latest/docs/reference/config/networking/virtual-service/#CorsPolicy
  # Takes precedence over base_cors_policy.
  custom_cors_policy: {}

# Basic mesh-related data.
mesh:
  enabled: true
  certmanager:
    enabled: true
  admin: {port: 1666 }
  image_version: latest
  # http keepalive timeout for incoming requests
  idle_timeout: "4.5s"
  # Port to listen to
  public_port: 1025 # arbitrary placeholder
  local_access_log_min_code: "200"
  # Headers to add to a local request,
  # in dictionary form.
  request_headers_to_add: []
  # Timeout of a request to the local service
  upstream_timeout: "300s"
  # Enabling telemetry, telemetry port.
  telemetry:
    enabled: true
    port: 1667
  extra_service_selector_labels:
    component: webserver
  resources:
    requests:
      cpu: 200m
      memory: 100Mi
    limits:
      cpu: 500m
      memory: 500Mi

# Mesh-related discovery
# TODO: move under mesh.* once we can
discovery:
  # List of listeners
  listeners: []

# Mesh related pure TCP proxies
tcp_proxy:
  listeners: []

# Should be provided by configuration management.
# See details of the structures in the comments
# In the configuration module.
services_proxy: ~
tcp_services_proxy: ~

common_images:
  statsd:
    exporter: prometheus-statsd-exporter:latest
# WARNING: If you want to enable the module,
# you will need to add a "statsd" stanza to monitoring
# see modules/base/values.yaml for reference.
  kerberos:
    image: repos/data-engineering/kerberos-kinit
    version: latest

docker:
  registry: docker-registry.wikimedia.org
  pull_policy: IfNotPresent
resources:
  replicas: 1

monitoring:
  # If enabled is true, monitoring annotations will be added to the deployment.
  enabled: true
  prometheus_port: 9102
  statsd:
    ipv4: true
    port: 9125
    prestop_sleep: 0
    requests:
      memory: 100Mi
      cpu: 100m
    limits:
      memory: 200Mi
      cpu: 200m
    # from https://github.com/blueswen/gunicorn-monitoring
    filename: files/statsd/prometheus-statsd.yaml

networkpolicy:
  egress:
    enabled: true

# Add here the list of kafka-clusters (by name) that the service will need to reach.
kafka:
  allowed_clusters: []

# Optional affinity settings
affinity: {}
#  affinity:
#    nodeAffinity:
#      requiredDuringSchedulingIgnoredDuringExecution:
#        nodeSelectorTerms:
#          - matchExpressions:
#              - key: some-key
#                operator: In
#                values:
#                  - some-value
#  nodeSelector:
#    node.kubernetes.io/some-key: some-value

# Cronjob definitions
# Here you can define your cronjobs
cronjobs: []
# Example of a job:
# - name: my-cron-hourly
#   enabled: true
#   command:
#      - /bin/cowsay
#      - "hello"
#   schedule: "@hourly" (defaults to @daily)
#   concurrency: Replace (defaults to "Forbid")
#   image_versioned: my-app:1.1.1 (defaults to the app used in the main application definition)
#   resources: (optional list of requests/limits for our cronjob; if not present will default to the application ones.)

# The set of external services to allow egress to
# Example:
# kafka:
# - main-codfw
# - main-eqiad
# presto:
# - analytics
#
# See https://wikitech.wikimedia.org/wiki/Kubernetes/Deployment_Charts#Enabling_egress_to_services_external_to_Kubernetes
# for the list of supported services
external_services: {}

pgServiceName: override_me
environmentName: override_me

# Definition of extra IP: hostnames to be injected in /etc/hosts
# Typically, we use this to force the reverse DNS resolution of an an-master host
# IP to its an-masterxxxx.eqiad.wmnet name and to avoid the coredns service ip reverse
# DNS, to validate Kerberos identity.
host_aliases: {}

devenv:
  enabled: false
  db:
    name: override_me
  kerberos:
    ticket_renewal_interval_minutes: 60

# These values define ssh configuration used by rsync/parallel-rsync
# to enable specific task pods running the repos/data-engineering/sync-utils image
# ssh access of to remote hosts, to be able to copy files.
rsync:
  enabled: false
  ssh:
    config: |
      Cipher es128-gcm@openssh.com
    # this dict contains dictionaries of the following structure
    # key: name # name of the target, to be used in kubernetes resource names. ex: pupperservers
    # value: dict componsed of the following keys:
    #   user: override_me # the ssh user used to ssh to the remote hosts
    #   private_key: {}  # the ssh secret key to ssh to the remote hosts. dict with keys `filename` and `content`
    #   known_hosts: override_me  # the content of the ssh known_hosts file
    #   hosts_cidrs: []  # the IP CIDRs of the remote hosts to be ssh-ed into
    targets: {}
