---
{{/*
  This configmap is used to assign the spark configuration key/values to
  environment variables that will be injected into the pod.
*/}}
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-history-config
  {{- include "base.meta.labels" . | indent 2 }}
  namespace: {{ .Release.Namespace }}
data:
    {{- range $k, $v := .Values.config.spark }}
    SPARK_JAVA_OPT_{{ upper $k }}: -D{{ $k }}={{ $v }}
    {{- end }}
    SPARK_DAEMON_MEMORY: {{ atoi (trimSuffix "Gi" $.Values.app.requests.memory) }}g {{/* used for both -Xms and -Xmx */}}

---
{{/*
  This configmap is used to define the Hadoop and HDFS configuration
  files, to be mounted in the pod.
*/}}
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-history-hadoop-sites-config
  {{- include "base.meta.labels" . | indent 2 }}
  namespace: {{ .Release.Namespace }}
data:
  core-site.xml: |
    <configuration>
    {{- range $k, $v := .Values.config.hadoop }}
    <property>
      <name>{{ $k }}</name>
      <value>{{ $v }}</value>
    </property>
    {{- end -}}
    </configuration>

  hdfs-site.xml: |
    <configuration>
    {{- range $k, $v := .Values.config.hdfs }}
    <property>
      <name>{{ $k }}</name>
      <value>{{ $v }}</value>
    </property>
    {{- end -}}
    </configuration>

---
{{/*
  This configmap is used to define the kerberos client configuration.
*/}}
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-history-kerberos-client-config
  {{- include "base.meta.labels" . | indent 2 }}
  namespace: {{ .Release.Namespace }}
data:
  krb5.conf: |
    [libdefaults]
      default_realm = WIKIMEDIA
      kdc_timesync = 1
      ccache_type = 4
      forwardable = true
      proxiable = true
      ticket_lifetime = 2d
      renew_lifetime = 14d
      dns_canonicalize_hostname = true
      rdns = false

    [realms]
      WIKIMEDIA = {
        kdc = krb1001.eqiad.wmnet
        kdc = krb1002.codfw.wmnet
        admin_server = krb1001.eqiad.wmnet
      }
    [domain_realm]
      .wikimedia = WIKIMEDIA
      wikimedia = WIKIMEDIA
---
{{ include "mesh.configuration.configmap" . }}

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-config
  {{- include "base.meta.labels" . | indent 2 }}
  namespace: {{ .Release.Namespace }}
data:
  {{/* taken from https://github.com/apache/spark/blob/master/conf/log4j2.properties.template */}}
  log4j2.properties: |
    # Set everything to be logged to the console
    rootLogger.level = {{ $.Values.config.logging.root.level }}
    rootLogger.appenderRef.stdout.ref = console

    # In the pattern layout configuration below, we specify an explicit `%ex` conversion
    # pattern for logging Throwables. If this was omitted, then (by default) Log4J would
    # implicitly add an `%xEx` conversion pattern which logs stacktraces with additional
    # class packaging information. That extra information can sometimes add a substantial
    # performance overhead, so we disable it in our default logging config.
    # For more information, see SPARK-39361.
    appender.console.type = Console
    appender.console.name = console
    appender.console.target = SYSTEM_ERR
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n%ex

    # Set the default spark-shell/spark-sql log level to WARN. When running the
    # spark-shell/spark-sql, the log level for these classes is used to overwrite
    # the root logger's log level, so that the user can have different defaults
    # for the shell and regular Spark apps.
    logger.repl.name = org.apache.spark.repl.Main
    logger.repl.level = warn

    logger.thriftserver.name = org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver
    logger.thriftserver.level = warn

    # Settings to quiet third party logs that are too verbose
    logger.jetty1.name = org.sparkproject.jetty
    logger.jetty1.level = warn
    logger.jetty2.name = org.sparkproject.jetty.util.component.AbstractLifeCycle
    logger.jetty2.level = error
    logger.replexprTyper.name = org.apache.spark.repl.SparkIMain$exprTyper
    logger.replexprTyper.level = info
    logger.replSparkILoopInterpreter.name = org.apache.spark.repl.SparkILoop$SparkILoopInterpreter
    logger.replSparkILoopInterpreter.level = info
    logger.parquet1.name = org.apache.parquet
    logger.parquet1.level = error
    logger.parquet2.name = parquet
    logger.parquet2.level = error

    # SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support
    logger.RetryingHMSHandler.name = org.apache.hadoop.hive.metastore.RetryingHMSHandler
    logger.RetryingHMSHandler.level = fatal
    logger.FunctionRegistry.name = org.apache.hadoop.hive.ql.exec.FunctionRegistry
    logger.FunctionRegistry.level = error

    # For deploying Spark ThriftServer
    # SPARK-34128: Suppress undesirable TTransportException warnings involved in THRIFT-4805
    appender.console.filter.1.type = RegexFilter
    appender.console.filter.1.regex = .*Thrift error occurred during processing of message.*
    appender.console.filter.1.onMatch = deny
    appender.console.filter.1.onMismatch = neutral
  spark-env.sh: |
    {{- range $k, $v := .Values.config.sparkEnv }}
    export {{ $k }}={{ $v }}
    {{- end -}}
