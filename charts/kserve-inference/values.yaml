helm_scaffold_version: 0.3 # This can be useful when backporting fixes.

inference:
  swift_s3_secret_name: "swift-s3-credentials"
  predictor:
    config:
      serviceAccountName: "kserve"
      dnsConfig:
        options:
          - name: ndots
            value: "2"
  transformer:
    config:
      dnsConfig:
        options:
          - name: ndots
            value: "2"

networkpolicy:
  egress:
    enabled: false

monitoring:
  enabled: false

# All the models are wrapped in a KServer python runtime (provided by KServe),
# that uses Tornado behind the scenes. By default we use port 8080.
# Knative-serving adds a container called 'queue-proxy' to every InferenceService
# pod, that is responsible to assess if traffic can reach the KServe container.
# To simplify our settings, we assume that if transformers are used, they will
# listen on the same default port as the predictors.
main_app:
  port: 8080
  queue_proxy:
    port: 8012
    metrics_port: 9090
    revision_metrics_port: 9091
  istio_sidecar:
    metrics_port: 15020